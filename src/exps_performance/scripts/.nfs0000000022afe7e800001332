#!/bin/sh
#SBATCH --job-name=gemma
#SBATCH --output=log/Gemmaoutput.txt
#SBATCH --error=log/Gemmaerror.txt
#SBATCH --nodes=1
#SBATCH --gres=gpu:8
#SBATCH --constraint=GA102GL     
#SBATCH --time=1-6:00:00

SEEDS=(0)
MODELS=(
  # "google/gemma-2-9b-it"
  # "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct" #timeout errs?
  # "Qwen/Qwen2.5-14B-Instruct"
  # "mistralai/Mistral-Small-24B-Instruct-2501"
  # meta-llama/Llama-3.1-8B-Instruct
  google/gemma-3-12b-it
)

for MODEL in ${MODELS[@]}; do
  for SEED in ${SEEDS[@]}; do
    pixi run python src/exps_performance/main.py \
      --root src/exps_performance/ \
      --backend vllm \
      --model ${MODEL} \
      --hf_dtype float16 \
      --hf_device_map auto \
      --vllm_tensor_parallel 8 \
      --n 12 --digits 2 4 8 16  --kinds  spp bsp edp gcp gcpd tsp tspd ksp msp gsm8k clrs30 add sub mul lcs rod knap ilp_assign ilp_partition ilp_prod \
      --exec_code --batch_size 64 --seed ${SEED} --controlled_sim
  done
done


