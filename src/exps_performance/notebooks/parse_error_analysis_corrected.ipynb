{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Error Analysis (CORRECTED): Relationship with Code vs NL Trend\n",
    "\n",
    "**IMPORTANT FIX**: The `code_parse_err` boolean field is misleading!\n",
    "- Many records have `code_parse_err=True` but `code_err_msg=\"ok,ok\"` AND `code_correct=True`\n",
    "- We use `code_err_msg` to detect REAL errors:\n",
    "  - \"ok\" or \"ok,ok\" → NO error\n",
    "  - Empty string or other values → Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "RESULTS_DIR = Path(\"../results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_real_error(err_msg):\n",
    "    \"\"\"Check if err_msg indicates a REAL error.\n",
    "    \n",
    "    Returns True if error, False if 'ok' (no error).\n",
    "    \"\"\"\n",
    "    if not err_msg or err_msg == '':\n",
    "        return True  # Empty = no response = error\n",
    "    err_msg = str(err_msg).lower().strip()\n",
    "    if err_msg in ['ok', 'ok,ok']:\n",
    "        return False  # \"ok\" means no error\n",
    "    return True\n",
    "\n",
    "# Load all results\n",
    "all_rows = []\n",
    "for jsonl_path in RESULTS_DIR.rglob(\"res.jsonl\"):\n",
    "    with open(jsonl_path) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                row = json.loads(line)\n",
    "                model = row.get('model', '').replace('openrouter/', '').replace('anthropic/', '').replace('google/', '').replace('openai/', '')\n",
    "                if not model:\n",
    "                    continue\n",
    "                all_rows.append({\n",
    "                    'model': model,\n",
    "                    'nl_correct': row.get('nl_correct', False),\n",
    "                    'code_correct': row.get('code_correct', False),\n",
    "                    'sim_correct': row.get('sim_correct', False),\n",
    "                    'nl_err': has_real_error(row.get('nl_err_msg', '')),\n",
    "                    'code_err': has_real_error(row.get('code_err_msg', '')),\n",
    "                    'sim_err': has_real_error(row.get('sim_err_msg', '')),\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "print(f\"Loaded {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Model Statistics (Corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-model stats with CORRECTED error detection\n",
    "model_stats = []\n",
    "for model, group in df.groupby('model'):\n",
    "    if len(group) < 50:\n",
    "        continue\n",
    "    stats = {\n",
    "        'model': model,\n",
    "        'n': len(group),\n",
    "        'nl_acc': group['nl_correct'].mean() * 100,\n",
    "        'code_acc': group['code_correct'].mean() * 100,\n",
    "        'sim_acc': group['sim_correct'].mean() * 100,\n",
    "        'nl_err%': group['nl_err'].mean() * 100,\n",
    "        'code_err%': group['code_err'].mean() * 100,\n",
    "        'sim_err%': group['sim_err'].mean() * 100,\n",
    "    }\n",
    "    stats['code_vs_nl'] = stats['code_acc'] - stats['nl_acc']\n",
    "    model_stats.append(stats)\n",
    "\n",
    "stats_df = pd.DataFrame(model_stats).sort_values('code_vs_nl', ascending=False)\n",
    "print(f\"Computed stats for {len(stats_df)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display table\n",
    "styled = stats_df.style.format({\n",
    "    'nl_acc': '{:.1f}%', 'code_acc': '{:.1f}%', 'sim_acc': '{:.1f}%',\n",
    "    'code_vs_nl': '{:+.1f}%',\n",
    "    'nl_err%': '{:.1f}%', 'code_err%': '{:.1f}%', 'sim_err%': '{:.1f}%',\n",
    "}).background_gradient(subset=['code_vs_nl'], cmap='RdYlGn', vmin=-50, vmax=50)\n",
    "styled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis (Corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = {}\n",
    "\n",
    "r, p = scipy_stats.pearsonr(stats_df['code_err%'], stats_df['code_vs_nl'])\n",
    "correlations['Code Err% vs Code>NL'] = {'r': r, 'p': p}\n",
    "\n",
    "r, p = scipy_stats.pearsonr(stats_df['nl_err%'], stats_df['code_vs_nl'])\n",
    "correlations['NL Err% vs Code>NL'] = {'r': r, 'p': p}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CORRELATION ANALYSIS (CORRECTED error detection)\")\n",
    "print(\"=\"*60)\n",
    "for name, vals in correlations.items():\n",
    "    sig = \"***\" if vals['p'] < 0.001 else \"**\" if vals['p'] < 0.01 else \"*\" if vals['p'] < 0.05 else \"\"\n",
    "    print(f\"{name:<30}: r = {vals['r']:+.3f}, p = {vals['p']:.4f} {sig}\")\n",
    "print(\"\\n* p < 0.05, ** p < 0.01, *** p < 0.001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Code Error vs Code>NL\n",
    "ax = axes[0]\n",
    "ax.scatter(stats_df['code_err%'], stats_df['code_vs_nl'], s=stats_df['n']/30, alpha=0.6, c='steelblue')\n",
    "z = np.polyfit(stats_df['code_err%'], stats_df['code_vs_nl'], 1)\n",
    "x_line = np.linspace(0, 100, 100)\n",
    "ax.plot(x_line, np.poly1d(z)(x_line), \"r--\", alpha=0.8, label=f'r={correlations[\"Code Err% vs Code>NL\"][\"r\"]:.3f}')\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xlabel('Code Error % (from err_msg)')\n",
    "ax.set_ylabel('Code - NL Accuracy Gap (%)')\n",
    "ax.set_title('Code Error vs Code>NL Gap')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: NL Error vs Code>NL (SIGNIFICANT!)\n",
    "ax = axes[1]\n",
    "ax.scatter(stats_df['nl_err%'], stats_df['code_vs_nl'], s=stats_df['n']/30, alpha=0.6, c='forestgreen')\n",
    "z = np.polyfit(stats_df['nl_err%'], stats_df['code_vs_nl'], 1)\n",
    "ax.plot(x_line, np.poly1d(z)(x_line), \"r--\", alpha=0.8, label=f'r={correlations[\"NL Err% vs Code>NL\"][\"r\"]:.3f}***')\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.set_xlabel('NL Error % (from err_msg)')\n",
    "ax.set_ylabel('Code - NL Accuracy Gap (%)')\n",
    "ax.set_title('NL Error vs Code>NL Gap (SIGNIFICANT)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('parse_error_correlation_corrected.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot by error category\n",
    "def categorize_err(rate):\n",
    "    if rate < 20: return '0-19% (Low)'\n",
    "    elif rate < 50: return '20-49% (Moderate)'\n",
    "    else: return '50%+ (High)'\n",
    "\n",
    "stats_df['err_cat'] = stats_df['code_err%'].apply(categorize_err)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "order = ['0-19% (Low)', '20-49% (Moderate)', '50%+ (High)']\n",
    "colors = ['#2ecc71', '#f1c40f', '#e74c3c']\n",
    "\n",
    "sns.boxplot(data=stats_df, x='err_cat', y='code_vs_nl', order=order, palette=colors, ax=ax)\n",
    "sns.stripplot(data=stats_df, x='err_cat', y='code_vs_nl', order=order, color='black', alpha=0.5, ax=ax)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
    "ax.set_xlabel('Code Error Category (Corrected)')\n",
    "ax.set_ylabel('Code - NL Accuracy Gap (%)')\n",
    "ax.set_title('Code>NL Gap by Error Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('parse_error_boxplot_corrected.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = stats_df.groupby('err_cat').agg({\n",
    "    'model': 'count',\n",
    "    'code_vs_nl': 'mean',\n",
    "    'code_acc': 'mean',\n",
    "    'nl_acc': 'mean'\n",
    "}).round(1)\n",
    "summary.columns = ['n_models', 'avg_code_vs_nl', 'avg_code_acc', 'avg_nl_acc']\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings (Corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS (CORRECTED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "r_nl = correlations['NL Err% vs Code>NL']['r']\n",
    "p_nl = correlations['NL Err% vs Code>NL']['p']\n",
    "r_code = correlations['Code Err% vs Code>NL']['r']\n",
    "p_code = correlations['Code Err% vs Code>NL']['p']\n",
    "\n",
    "low_err_avg = stats_df[stats_df['code_err%'] < 20]['code_vs_nl'].mean()\n",
    "high_err_avg = stats_df[stats_df['code_err%'] >= 50]['code_vs_nl'].mean()\n",
    "\n",
    "print(f\"\"\"\n",
    "1. SIGNIFICANT CORRELATION: NL Error% vs Code>NL\n",
    "   r = {r_nl:+.3f}, p = {p_nl:.4f} ***\n",
    "   Higher NL error → Higher Code>NL gap\n",
    "   Interpretation: When NL fails, code can still work → Code advantage\n",
    "\n",
    "2. NON-SIGNIFICANT: Code Error% vs Code>NL\n",
    "   r = {r_code:+.3f}, p = {p_code:.4f}\n",
    "   Code errors alone don't strongly predict the gap\n",
    "\n",
    "3. BY ERROR CATEGORY:\n",
    "   - Low code error (<20%):  avg Code>NL = {low_err_avg:+.1f}%\n",
    "   - High code error (50%+): avg Code>NL = {high_err_avg:+.1f}%\n",
    "\n",
    "4. BEST MODELS FOR CLEAN ANALYSIS (low code error <20%):\n",
    "\"\"\")\n",
    "\n",
    "low_err_models = stats_df[stats_df['code_err%'] < 20].sort_values('code_vs_nl', ascending=False)\n",
    "for _, row in low_err_models.iterrows():\n",
    "    print(f\"   - {row['model']:<35} Code>NL: {row['code_vs_nl']:+.1f}%, Code Err: {row['code_err%']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "stats_df.to_csv('model_parse_error_stats_corrected.csv', index=False)\n",
    "print(\"Saved to model_parse_error_stats_corrected.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
