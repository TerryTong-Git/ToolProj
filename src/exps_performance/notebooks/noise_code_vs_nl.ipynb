{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Execution vs NL: Noise Robustness Analysis\n",
    "\n",
    "This notebook analyzes whether code execution is similar to or as good as NL (natural language) at handling noise for algorithmic tasks.\n",
    "\n",
    "**Hypothesis**: Code-based reasoning is at least as robust to input noise as NL reasoning for algorithmic tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Publication-quality settings\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "plt.rcParams[\"savefig.dpi\"] = 300\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"axes.labelsize\"] = 14\n",
    "plt.rcParams[\"axes.titlesize\"] = 16\n",
    "plt.rcParams[\"legend.fontsize\"] = 12\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 result files\n",
      "  ../results_noise/anthropic-claude-haiku-4.5_seed1_noise_20260107_231724.json\n"
     ]
    }
   ],
   "source": [
    "# Configure paths (search a few likely locations)\n",
    "CANDIDATE_DIRS = [\n",
    "    Path(\"../results_noise_code_vs_nl\"),\n",
    "    Path(\"../results_noise\"),\n",
    "]\n",
    "FILES = []\n",
    "for d in CANDIDATE_DIRS:\n",
    "    if d.exists():\n",
    "        FILES.extend(list(d.glob(\"*.json\")))\n",
    "FILES = sorted(set(FILES))\n",
    "print(f\"Found {len(FILES)} result files\")\n",
    "for f in FILES[:5]:\n",
    "    print(f\"  {f}\")\n",
    "if len(FILES) > 5:\n",
    "    print(f\"  ... and {len(FILES) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 160 records from 1 files\n",
      "Models: ['anthropic/claude-haiku-4.5']\n",
      "Arms: ['code', 'controlsim', 'nl', 'sim']\n",
      "Noise types: ['gaussian', 'uniform']\n",
      "Sigma levels: [0.0, 0.25, 0.5, 0.75, 1.0]\n",
      "Problem kinds: ['add', 'lcs', 'mul', 'sub']\n"
     ]
    }
   ],
   "source": [
    "def load_noise_results(files):\n",
    "    \"\"\"Load all noise experiment results from JSON files.\"\"\"\n",
    "    rows = []\n",
    "    summaries = []\n",
    "    \n",
    "    for fp in files:\n",
    "        try:\n",
    "            payload = json.loads(fp.read_text())\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not parse {fp}\")\n",
    "            continue\n",
    "        \n",
    "        if not payload:\n",
    "            continue\n",
    "        \n",
    "        # Last element is summary metadata, rest are records\n",
    "        *records, summary = payload\n",
    "        if records:\n",
    "            df = pd.DataFrame.from_records(records)\n",
    "            # Add metadata from summary to each record\n",
    "            df[\"model\"] = summary.get(\"model\", \"unknown\")\n",
    "            df[\"seed\"] = summary.get(\"seed\", 0)\n",
    "            df[\"source\"] = fp.name\n",
    "            rows.append(df)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    data = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "    meta = pd.DataFrame.from_records(summaries) if summaries else pd.DataFrame()\n",
    "    return data, meta\n",
    "\n",
    "# Load data\n",
    "if not FILES:\n",
    "    print(\"No noise result files found!\")\n",
    "    print(\"Generate results first via:\")\n",
    "    print(\"  bash src/exps_performance/scripts/noise_code_vs_nl.sh\")\n",
    "    print(\"Or:\")\n",
    "    print(\"  bash src/exps_performance/scripts/noise.sh\")\n",
    "    data = pd.DataFrame()\n",
    "    meta = pd.DataFrame()\n",
    "else:\n",
    "    data, meta = load_noise_results(FILES)\n",
    "    print(f\"Loaded {len(data)} records from {len(FILES)} files\")\n",
    "    \n",
    "# Show summary only if data exists and has required columns\n",
    "if not data.empty and \"model\" in data.columns:\n",
    "    print(f\"Models: {sorted(data['model'].unique())}\")\n",
    "    print(f\"Arms: {sorted(data['arm'].unique())}\")\n",
    "    print(f\"Noise types: {sorted(data['noise_type'].unique())}\")\n",
    "    print(f\"Sigma levels: {sorted(data['sigma'].unique())}\")\n",
    "    print(f\"Problem kinds: {sorted(data['kind'].unique())}\")\n",
    "elif data.empty:\n",
    "    print(\"DataFrame is empty - check if JSON files contain records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 80 records (nl + code arms only)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>arm</th>\n",
       "      <th>noise_type</th>\n",
       "      <th>sigma</th>\n",
       "      <th>model</th>\n",
       "      <th>seed</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>add</td>\n",
       "      <td>1.00</td>\n",
       "      <td>nl</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anthropic/claude-haiku-4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic-claude-haiku-4.5_seed1_noise_2026010...</td>\n",
       "      <td>Arithmetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lcs</td>\n",
       "      <td>1.00</td>\n",
       "      <td>nl</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anthropic/claude-haiku-4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic-claude-haiku-4.5_seed1_noise_2026010...</td>\n",
       "      <td>DP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mul</td>\n",
       "      <td>0.75</td>\n",
       "      <td>nl</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anthropic/claude-haiku-4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic-claude-haiku-4.5_seed1_noise_2026010...</td>\n",
       "      <td>Arithmetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub</td>\n",
       "      <td>1.00</td>\n",
       "      <td>nl</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anthropic/claude-haiku-4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic-claude-haiku-4.5_seed1_noise_2026010...</td>\n",
       "      <td>Arithmetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>add</td>\n",
       "      <td>1.00</td>\n",
       "      <td>code</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anthropic/claude-haiku-4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic-claude-haiku-4.5_seed1_noise_2026010...</td>\n",
       "      <td>Arithmetic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kind  accuracy   arm noise_type  sigma                       model  seed  \\\n",
       "0   add      1.00    nl   gaussian    0.0  anthropic/claude-haiku-4.5     1   \n",
       "1   lcs      1.00    nl   gaussian    0.0  anthropic/claude-haiku-4.5     1   \n",
       "2   mul      0.75    nl   gaussian    0.0  anthropic/claude-haiku-4.5     1   \n",
       "3   sub      1.00    nl   gaussian    0.0  anthropic/claude-haiku-4.5     1   \n",
       "12  add      1.00  code   gaussian    0.0  anthropic/claude-haiku-4.5     1   \n",
       "\n",
       "                                               source    category  \n",
       "0   anthropic-claude-haiku-4.5_seed1_noise_2026010...  Arithmetic  \n",
       "1   anthropic-claude-haiku-4.5_seed1_noise_2026010...          DP  \n",
       "2   anthropic-claude-haiku-4.5_seed1_noise_2026010...  Arithmetic  \n",
       "3   anthropic-claude-haiku-4.5_seed1_noise_2026010...  Arithmetic  \n",
       "12  anthropic-claude-haiku-4.5_seed1_noise_2026010...  Arithmetic  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic cleaning/casting and filtering\n",
    "if not data.empty:\n",
    "    data[\"sigma\"] = data[\"sigma\"].astype(float)\n",
    "    data[\"accuracy\"] = data[\"accuracy\"].astype(float)\n",
    "    data[\"noise_type\"] = data[\"noise_type\"].astype(str)\n",
    "    data[\"arm\"] = data[\"arm\"].astype(str)\n",
    "    data[\"kind\"] = data[\"kind\"].astype(str)\n",
    "    \n",
    "    # Filter to Code and NL arms only\n",
    "    data = data[data[\"arm\"].isin([\"nl\", \"code\"])].copy()\n",
    "    \n",
    "    # Add problem category\n",
    "    category_map = {\n",
    "        \"add\": \"Arithmetic\", \"sub\": \"Arithmetic\", \"mul\": \"Arithmetic\",\n",
    "        \"lcs\": \"DP\", \"knap\": \"DP\", \"rod\": \"DP\",\n",
    "        \"ilp_assign\": \"ILP\", \"ilp_prod\": \"ILP\", \"ilp_partition\": \"ILP\",\n",
    "        \"tsp\": \"NP-Hard\", \"gcp\": \"NP-Hard\", \"spp\": \"NP-Hard\",\n",
    "        \"bsp\": \"NP-Hard\", \"edp\": \"NP-Hard\", \"msp\": \"NP-Hard\",\n",
    "        \"ksp\": \"NP-Hard\", \"tspd\": \"NP-Hard\", \"gcpd\": \"NP-Hard\",\n",
    "        \"clrs30\": \"CLRS\",\n",
    "    }\n",
    "    data[\"category\"] = data[\"kind\"].map(category_map).fillna(\"Other\")\n",
    "    \n",
    "    print(f\"Filtered to {len(data)} records (nl + code arms only)\")\n",
    "    display(data.head())\n",
    "else:\n",
    "    print(\"No data to process. Run experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aggregated Accuracy vs Noise Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not data.empty:\n    # Aggregate across all problem types, models, and seeds\n    agg = data.groupby([\"arm\", \"sigma\"])[\"accuracy\"].agg([\"mean\", \"std\", \"count\"]).reset_index()\n    agg[\"se\"] = agg[\"std\"] / np.sqrt(agg[\"count\"])\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n\n    colors = {\"nl\": \"#1f77b4\", \"code\": \"#ff7f0e\"}\n    labels = {\"nl\": \"NL (Arm 1)\", \"code\": \"Code Exec (Arm 3)\"}\n    markers = {\"nl\": \"o\", \"code\": \"s\"}\n\n    for arm in [\"nl\", \"code\"]:\n        arm_data = agg[agg[\"arm\"] == arm].sort_values(\"sigma\")\n        if arm_data.empty:\n            continue\n        ax.errorbar(\n            arm_data[\"sigma\"],\n            arm_data[\"mean\"],\n            yerr=1.96 * arm_data[\"se\"],\n            label=labels[arm],\n            color=colors[arm],\n            marker=markers[arm],\n            markersize=10,\n            linewidth=2,\n            capsize=5,\n        )\n\n    ax.set_xlabel(\"Noise Level (σ)\")\n    ax.set_ylabel(\"Accuracy\")\n    ax.set_ylim(0, 1.05)\n    ax.legend(loc=\"lower left\")\n    ax.set_title(\"Code Execution vs NL: Accuracy under Noise\")\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No data available for plotting.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accuracy by Noise Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not data.empty:\n    noise_types = sorted(data[\"noise_type\"].unique())\n    n_types = len(noise_types)\n    \n    n_cols = min(3, n_types)\n    n_rows = (n_types + n_cols - 1) // n_cols\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows), sharey=True, squeeze=False)\n    axes = axes.flatten()\n\n    colors = {\"nl\": \"#1f77b4\", \"code\": \"#ff7f0e\"}\n    labels = {\"nl\": \"NL\", \"code\": \"Code Exec\"}\n\n    for idx, noise_type in enumerate(noise_types):\n        ax = axes[idx]\n        subset = data[data[\"noise_type\"] == noise_type]\n        \n        agg = subset.groupby([\"arm\", \"sigma\"])[\"accuracy\"].agg([\"mean\", \"std\", \"count\"]).reset_index()\n        agg[\"se\"] = agg[\"std\"] / np.sqrt(agg[\"count\"])\n        \n        for arm in [\"nl\", \"code\"]:\n            arm_data = agg[agg[\"arm\"] == arm].sort_values(\"sigma\")\n            if arm_data.empty:\n                continue\n            ax.errorbar(\n                arm_data[\"sigma\"],\n                arm_data[\"mean\"],\n                yerr=1.96 * arm_data[\"se\"],\n                label=labels[arm],\n                color=colors[arm],\n                marker=\"o\" if arm == \"nl\" else \"s\",\n                markersize=8,\n                linewidth=2,\n                capsize=4,\n            )\n        \n        ax.set_title(noise_type.capitalize())\n        ax.set_xlabel(\"σ\")\n        ax.set_ylim(0, 1.05)\n        ax.grid(True, alpha=0.3)\n        \n        if idx == 0:\n            ax.set_ylabel(\"Accuracy\")\n            ax.legend(loc=\"lower left\")\n\n    for idx in range(n_types, len(axes)):\n        axes[idx].set_visible(False)\n\n    plt.suptitle(\"Accuracy under Different Noise Types: Code Exec vs NL\", y=1.02, fontsize=16)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No data available for plotting.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Accuracy by Problem Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not data.empty:\n    categories = sorted(data[\"category\"].unique())\n    n_cats = len(categories)\n\n    n_cols = min(3, n_cats)\n    n_rows = (n_cats + n_cols - 1) // n_cols\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows), sharey=True, squeeze=False)\n    axes = axes.flatten()\n\n    colors = {\"nl\": \"#1f77b4\", \"code\": \"#ff7f0e\"}\n    labels = {\"nl\": \"NL\", \"code\": \"Code Exec\"}\n\n    for idx, category in enumerate(categories):\n        ax = axes[idx]\n        subset = data[data[\"category\"] == category]\n        \n        agg = subset.groupby([\"arm\", \"sigma\"])[\"accuracy\"].agg([\"mean\", \"std\", \"count\"]).reset_index()\n        agg[\"se\"] = agg[\"std\"] / np.sqrt(agg[\"count\"])\n        \n        for arm in [\"nl\", \"code\"]:\n            arm_data = agg[agg[\"arm\"] == arm].sort_values(\"sigma\")\n            if arm_data.empty:\n                continue\n            ax.errorbar(\n                arm_data[\"sigma\"],\n                arm_data[\"mean\"],\n                yerr=1.96 * arm_data[\"se\"],\n                label=labels[arm],\n                color=colors[arm],\n                marker=\"o\" if arm == \"nl\" else \"s\",\n                markersize=8,\n                linewidth=2,\n                capsize=4,\n            )\n        \n        ax.set_title(category)\n        ax.set_xlabel(\"σ\")\n        ax.set_ylim(0, 1.05)\n        ax.grid(True, alpha=0.3)\n        \n        if idx == 0:\n            ax.set_ylabel(\"Accuracy\")\n            ax.legend(loc=\"lower left\")\n\n    for idx in range(n_cats, len(axes)):\n        axes[idx].set_visible(False)\n\n    plt.suptitle(\"Accuracy by Problem Category: Code Exec vs NL\", y=1.02, fontsize=16)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No data available for plotting.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_statistical_tests(data: pd.DataFrame, delta: float = 0.05) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run statistical tests to determine if Code is \"similar to or as good as\" NL.\n",
    "    \n",
    "    Tests:\n",
    "    1. Paired t-test: H0: mean(Code) = mean(NL)\n",
    "    2. Wilcoxon signed-rank test: Non-parametric alternative\n",
    "    3. Non-inferiority test: H0: mean(Code) - mean(NL) < -delta\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Determine grouping columns based on what's available\n",
    "    group_cols = [\"kind\"]\n",
    "    if \"model\" in data.columns:\n",
    "        group_cols.append(\"model\")\n",
    "    if \"seed\" in data.columns:\n",
    "        group_cols.append(\"seed\")\n",
    "    \n",
    "    for noise_type in data[\"noise_type\"].unique():\n",
    "        for sigma in sorted(data[\"sigma\"].unique()):\n",
    "            subset = data[(data[\"noise_type\"] == noise_type) & (data[\"sigma\"] == sigma)]\n",
    "            \n",
    "            # Pivot to get paired observations\n",
    "            try:\n",
    "                pivot = subset.pivot_table(\n",
    "                    index=group_cols,\n",
    "                    columns=\"arm\",\n",
    "                    values=\"accuracy\"\n",
    "                ).dropna()\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not pivot for {noise_type}, sigma={sigma}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            if len(pivot) < 3 or \"nl\" not in pivot.columns or \"code\" not in pivot.columns:\n",
    "                continue\n",
    "            \n",
    "            nl_acc = pivot[\"nl\"].values\n",
    "            code_acc = pivot[\"code\"].values\n",
    "            diff = code_acc - nl_acc\n",
    "            \n",
    "            # Paired t-test\n",
    "            t_stat, t_pval = stats.ttest_rel(code_acc, nl_acc)\n",
    "            \n",
    "            # Wilcoxon\n",
    "            try:\n",
    "                w_stat, w_pval = stats.wilcoxon(code_acc, nl_acc)\n",
    "            except ValueError:\n",
    "                w_stat, w_pval = np.nan, np.nan\n",
    "            \n",
    "            # Non-inferiority\n",
    "            ni_t_stat, ni_pval_two = stats.ttest_1samp(diff + delta, 0)\n",
    "            ni_pval = ni_pval_two / 2 if ni_t_stat > 0 else 1 - ni_pval_two / 2\n",
    "            \n",
    "            # Cohen's d\n",
    "            pooled_std = np.sqrt((np.var(nl_acc, ddof=1) + np.var(code_acc, ddof=1)) / 2)\n",
    "            cohens_d = np.mean(diff) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                \"noise_type\": noise_type,\n",
    "                \"sigma\": sigma,\n",
    "                \"n_pairs\": len(pivot),\n",
    "                \"mean_nl\": np.mean(nl_acc),\n",
    "                \"mean_code\": np.mean(code_acc),\n",
    "                \"mean_diff\": np.mean(diff),\n",
    "                \"std_diff\": np.std(diff, ddof=1),\n",
    "                \"cohens_d\": cohens_d,\n",
    "                \"t_pval\": t_pval,\n",
    "                \"wilcoxon_pval\": w_pval,\n",
    "                \"noninferiority_pval\": ni_pval,\n",
    "                \"code_noninferior\": ni_pval < 0.05,\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if not data.empty:\n",
    "    stats_df = run_statistical_tests(data)\n",
    "    if not stats_df.empty:\n",
    "        display(stats_df)\n",
    "    else:\n",
    "        print(\"No statistical results - need both 'nl' and 'code' arms with sufficient data\")\n",
    "else:\n",
    "    stats_df = pd.DataFrame()\n",
    "    print(\"No data available for statistical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not stats_df.empty:\n",
    "    # Statistical summary heatmaps\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Mean difference\n",
    "    pivot_diff = stats_df.pivot(index=\"noise_type\", columns=\"sigma\", values=\"mean_diff\")\n",
    "    sns.heatmap(pivot_diff, ax=axes[0], cmap=\"RdYlGn\", center=0, annot=True, fmt=\".3f\",\n",
    "                cbar_kws={\"label\": \"Mean Diff (Code - NL)\"})\n",
    "    axes[0].set_title(\"Accuracy Difference: Code - NL\")\n",
    "    axes[0].set_xlabel(\"Noise Level (σ)\")\n",
    "    axes[0].set_ylabel(\"Noise Type\")\n",
    "\n",
    "    # Non-inferiority p-values\n",
    "    pivot_pval = stats_df.pivot(index=\"noise_type\", columns=\"sigma\", values=\"noninferiority_pval\")\n",
    "    sns.heatmap(pivot_pval, ax=axes[1], cmap=\"RdYlGn_r\", vmin=0, vmax=0.1, annot=True, fmt=\".3f\",\n",
    "                cbar_kws={\"label\": \"p-value\"})\n",
    "    axes[1].set_title(\"Non-inferiority Test (δ=0.05)\")\n",
    "    axes[1].set_xlabel(\"Noise Level (σ)\")\n",
    "    axes[1].set_ylabel(\"Noise Type\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No statistical results to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Effect Size Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not stats_df.empty:\n",
    "    # Cohen's d heatmap\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    pivot_d = stats_df.pivot(index=\"noise_type\", columns=\"sigma\", values=\"cohens_d\")\n",
    "    sns.heatmap(pivot_d, ax=ax, cmap=\"RdYlGn\", center=0, annot=True, fmt=\".2f\",\n",
    "                cbar_kws={\"label\": \"Cohen's d\"}, vmin=-1, vmax=1)\n",
    "    ax.set_title(\"Effect Size (Cohen's d): Code - NL\\n(+ve = Code better)\")\n",
    "    ax.set_xlabel(\"Noise Level (σ)\")\n",
    "    ax.set_ylabel(\"Noise Type\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No statistical results to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not stats_df.empty:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    noninferior_count = stats_df[\"code_noninferior\"].sum()\n",
    "    total_tests = len(stats_df)\n",
    "    print(f\"\\nNon-inferiority established in {noninferior_count}/{total_tests} conditions\")\n",
    "    print(f\"(Code is within 5% of NL accuracy at α=0.05)\")\n",
    "\n",
    "    # Bonferroni correction\n",
    "    bonferroni_alpha = 0.05 / total_tests if total_tests > 0 else 0.05\n",
    "    bonferroni_pass = (stats_df[\"noninferiority_pval\"] < bonferroni_alpha).sum()\n",
    "    print(f\"\\nWith Bonferroni correction (α={bonferroni_alpha:.4f}):\")\n",
    "    print(f\"  Non-inferiority established in {bonferroni_pass}/{total_tests} conditions\")\n",
    "\n",
    "    # Cases where Code outperforms NL\n",
    "    better = stats_df[stats_df[\"mean_diff\"] > 0]\n",
    "    print(f\"\\nCode outperforms NL in {len(better)}/{total_tests} conditions\")\n",
    "\n",
    "    # Overall verdict\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VERDICT\")\n",
    "    print(\"=\" * 60)\n",
    "    if total_tests == 0:\n",
    "        print(\"INSUFFICIENT DATA: Need more test conditions to draw conclusions.\")\n",
    "    elif noninferior_count >= total_tests * 0.8:\n",
    "        print(\"SUPPORTED: Code execution is similar to or as good as NL at handling noise.\")\n",
    "    elif noninferior_count >= total_tests * 0.5:\n",
    "        print(\"PARTIALLY SUPPORTED: Code is non-inferior in majority of conditions.\")\n",
    "    else:\n",
    "        print(\"NOT SUPPORTED: Code underperforms NL in majority of noise conditions.\")\n",
    "else:\n",
    "    print(\"No data available for summary.\")\n",
    "    print(\"\\nTo generate data, run:\")\n",
    "    print(\"  bash src/exps_performance/scripts/noise_code_vs_nl.sh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}