{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arc_challenge</th>\n",
       "      <th>arc_easy</th>\n",
       "      <th>bbh</th>\n",
       "      <th>bbh_cot_fewshot_boolean_expressions</th>\n",
       "      <th>bbh_cot_fewshot_causal_judgement</th>\n",
       "      <th>bbh_cot_fewshot_date_understanding</th>\n",
       "      <th>bbh_cot_fewshot_disambiguation_qa</th>\n",
       "      <th>bbh_cot_fewshot_dyck_languages</th>\n",
       "      <th>bbh_cot_fewshot_formal_fallacies</th>\n",
       "      <th>bbh_cot_fewshot_geometric_shapes</th>\n",
       "      <th>...</th>\n",
       "      <th>mmlu_pro_economics</th>\n",
       "      <th>mmlu_pro_engineering</th>\n",
       "      <th>mmlu_pro_health</th>\n",
       "      <th>mmlu_pro_history</th>\n",
       "      <th>mmlu_pro_law</th>\n",
       "      <th>mmlu_pro_math</th>\n",
       "      <th>mmlu_pro_other</th>\n",
       "      <th>mmlu_pro_philosophy</th>\n",
       "      <th>mmlu_pro_physics</th>\n",
       "      <th>mmlu_pro_psychology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allenai/Llama-3.1-Tulu-3-8B</th>\n",
       "      <td>{'alias': 'arc_challenge', 'acc,none': 0.56569...</td>\n",
       "      <td>{'alias': 'arc_easy', 'acc,none': 0.8392255892...</td>\n",
       "      <td>{'exact_match,get-answer': 0.6891414529258179,...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_boolean_expressi...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_causal_judgement...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_date_understandi...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_disambiguation_q...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_dyck_languages',...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_formal_fallacies...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_geometric_shapes...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'alias': ' - economics', 'exact_match,custom-...</td>\n",
       "      <td>{'alias': ' - engineering', 'exact_match,custo...</td>\n",
       "      <td>{'alias': ' - health', 'exact_match,custom-ext...</td>\n",
       "      <td>{'alias': ' - history', 'exact_match,custom-ex...</td>\n",
       "      <td>{'alias': ' - law', 'exact_match,custom-extrac...</td>\n",
       "      <td>{'alias': ' - math', 'exact_match,custom-extra...</td>\n",
       "      <td>{'alias': ' - other', 'exact_match,custom-extr...</td>\n",
       "      <td>{'alias': ' - philosophy', 'exact_match,custom...</td>\n",
       "      <td>{'alias': ' - physics', 'exact_match,custom-ex...</td>\n",
       "      <td>{'alias': ' - psychology', 'exact_match,custom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allenai/OLMo-2-1124-7B-Instruct</th>\n",
       "      <td>{'alias': 'arc_challenge', 'acc,none': 0.55119...</td>\n",
       "      <td>{'alias': 'arc_easy', 'acc,none': 0.8354377104...</td>\n",
       "      <td>{'exact_match,get-answer': 0.4904008600829366,...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_boolean_expressi...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_causal_judgement...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_date_understandi...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_disambiguation_q...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_dyck_languages',...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_formal_fallacies...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_geometric_shapes...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'alias': ' - economics', 'exact_match,custom-...</td>\n",
       "      <td>{'alias': ' - engineering', 'exact_match,custo...</td>\n",
       "      <td>{'alias': ' - health', 'exact_match,custom-ext...</td>\n",
       "      <td>{'alias': ' - history', 'exact_match,custom-ex...</td>\n",
       "      <td>{'alias': ' - law', 'exact_match,custom-extrac...</td>\n",
       "      <td>{'alias': ' - math', 'exact_match,custom-extra...</td>\n",
       "      <td>{'alias': ' - other', 'exact_match,custom-extr...</td>\n",
       "      <td>{'alias': ' - philosophy', 'exact_match,custom...</td>\n",
       "      <td>{'alias': ' - physics', 'exact_match,custom-ex...</td>\n",
       "      <td>{'alias': ' - psychology', 'exact_match,custom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marin-community/marin-8b-instruct</th>\n",
       "      <td>{'alias': 'arc_challenge', 'acc,none': 0.53071...</td>\n",
       "      <td>{'alias': 'arc_easy', 'acc,none': 0.8152356902...</td>\n",
       "      <td>{'exact_match,get-answer': 0.26432191675625866...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_boolean_expressi...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_causal_judgement...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_date_understandi...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_disambiguation_q...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_dyck_languages',...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_formal_fallacies...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_geometric_shapes...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'alias': ' - economics', 'exact_match,custom-...</td>\n",
       "      <td>{'alias': ' - engineering', 'exact_match,custo...</td>\n",
       "      <td>{'alias': ' - health', 'exact_match,custom-ext...</td>\n",
       "      <td>{'alias': ' - history', 'exact_match,custom-ex...</td>\n",
       "      <td>{'alias': ' - law', 'exact_match,custom-extrac...</td>\n",
       "      <td>{'alias': ' - math', 'exact_match,custom-extra...</td>\n",
       "      <td>{'alias': ' - other', 'exact_match,custom-extr...</td>\n",
       "      <td>{'alias': ' - philosophy', 'exact_match,custom...</td>\n",
       "      <td>{'alias': ' - physics', 'exact_match,custom-ex...</td>\n",
       "      <td>{'alias': ' - psychology', 'exact_match,custom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistralai/Mistral-Small-Instruct-2409</th>\n",
       "      <td>{'alias': 'arc_challenge', 'acc,none': 0.58959...</td>\n",
       "      <td>{'alias': 'arc_easy', 'acc,none': 0.8472222222...</td>\n",
       "      <td>{'exact_match,get-answer': 0.7716172630932269,...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_boolean_expressi...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_causal_judgement...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_date_understandi...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_disambiguation_q...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_dyck_languages',...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_formal_fallacies...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_geometric_shapes...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'alias': ' - economics', 'exact_match,custom-...</td>\n",
       "      <td>{'alias': ' - engineering', 'exact_match,custo...</td>\n",
       "      <td>{'alias': ' - health', 'exact_match,custom-ext...</td>\n",
       "      <td>{'alias': ' - history', 'exact_match,custom-ex...</td>\n",
       "      <td>{'alias': ' - law', 'exact_match,custom-extrac...</td>\n",
       "      <td>{'alias': ' - math', 'exact_match,custom-extra...</td>\n",
       "      <td>{'alias': ' - other', 'exact_match,custom-extr...</td>\n",
       "      <td>{'alias': ' - philosophy', 'exact_match,custom...</td>\n",
       "      <td>{'alias': ' - physics', 'exact_match,custom-ex...</td>\n",
       "      <td>{'alias': ' - psychology', 'exact_match,custom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ibm-granite/granite-3.2-8b-instruct</th>\n",
       "      <td>{'alias': 'arc_challenge', 'acc,none': 0.51365...</td>\n",
       "      <td>{'alias': 'arc_easy', 'acc,none': 0.8017676767...</td>\n",
       "      <td>{'exact_match,get-answer': 0.711411457533405, ...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_boolean_expressi...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_causal_judgement...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_date_understandi...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_disambiguation_q...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_dyck_languages',...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_formal_fallacies...</td>\n",
       "      <td>{'alias': ' - bbh_cot_fewshot_geometric_shapes...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'alias': ' - economics', 'exact_match,custom-...</td>\n",
       "      <td>{'alias': ' - engineering', 'exact_match,custo...</td>\n",
       "      <td>{'alias': ' - health', 'exact_match,custom-ext...</td>\n",
       "      <td>{'alias': ' - history', 'exact_match,custom-ex...</td>\n",
       "      <td>{'alias': ' - law', 'exact_match,custom-extrac...</td>\n",
       "      <td>{'alias': ' - math', 'exact_match,custom-extra...</td>\n",
       "      <td>{'alias': ' - other', 'exact_match,custom-extr...</td>\n",
       "      <td>{'alias': ' - philosophy', 'exact_match,custom...</td>\n",
       "      <td>{'alias': ' - physics', 'exact_match,custom-ex...</td>\n",
       "      <td>{'alias': ' - psychology', 'exact_match,custom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           arc_challenge  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': 'arc_challenge', 'acc,none': 0.56569...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': 'arc_challenge', 'acc,none': 0.55119...   \n",
       "marin-community/marin-8b-instruct      {'alias': 'arc_challenge', 'acc,none': 0.53071...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': 'arc_challenge', 'acc,none': 0.58959...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': 'arc_challenge', 'acc,none': 0.51365...   \n",
       "\n",
       "                                                                                arc_easy  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': 'arc_easy', 'acc,none': 0.8392255892...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': 'arc_easy', 'acc,none': 0.8354377104...   \n",
       "marin-community/marin-8b-instruct      {'alias': 'arc_easy', 'acc,none': 0.8152356902...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': 'arc_easy', 'acc,none': 0.8472222222...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': 'arc_easy', 'acc,none': 0.8017676767...   \n",
       "\n",
       "                                                                                     bbh  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'exact_match,get-answer': 0.6891414529258179,...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'exact_match,get-answer': 0.4904008600829366,...   \n",
       "marin-community/marin-8b-instruct      {'exact_match,get-answer': 0.26432191675625866...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'exact_match,get-answer': 0.7716172630932269,...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'exact_match,get-answer': 0.711411457533405, ...   \n",
       "\n",
       "                                                     bbh_cot_fewshot_boolean_expressions  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - bbh_cot_fewshot_boolean_expressi...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - bbh_cot_fewshot_boolean_expressi...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - bbh_cot_fewshot_boolean_expressi...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - bbh_cot_fewshot_boolean_expressi...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - bbh_cot_fewshot_boolean_expressi...   \n",
       "\n",
       "                                                        bbh_cot_fewshot_causal_judgement  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - bbh_cot_fewshot_causal_judgement...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - bbh_cot_fewshot_causal_judgement...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - bbh_cot_fewshot_causal_judgement...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - bbh_cot_fewshot_causal_judgement...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - bbh_cot_fewshot_causal_judgement...   \n",
       "\n",
       "                                                      bbh_cot_fewshot_date_understanding  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - bbh_cot_fewshot_date_understandi...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - bbh_cot_fewshot_date_understandi...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - bbh_cot_fewshot_date_understandi...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - bbh_cot_fewshot_date_understandi...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - bbh_cot_fewshot_date_understandi...   \n",
       "\n",
       "                                                       bbh_cot_fewshot_disambiguation_qa  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - bbh_cot_fewshot_disambiguation_q...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - bbh_cot_fewshot_disambiguation_q...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - bbh_cot_fewshot_disambiguation_q...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - bbh_cot_fewshot_disambiguation_q...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - bbh_cot_fewshot_disambiguation_q...   \n",
       "\n",
       "                                                          bbh_cot_fewshot_dyck_languages  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - bbh_cot_fewshot_dyck_languages',...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - bbh_cot_fewshot_dyck_languages',...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - bbh_cot_fewshot_dyck_languages',...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - bbh_cot_fewshot_dyck_languages',...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - bbh_cot_fewshot_dyck_languages',...   \n",
       "\n",
       "                                                        bbh_cot_fewshot_formal_fallacies  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - bbh_cot_fewshot_formal_fallacies...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - bbh_cot_fewshot_formal_fallacies...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - bbh_cot_fewshot_formal_fallacies...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - bbh_cot_fewshot_formal_fallacies...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - bbh_cot_fewshot_formal_fallacies...   \n",
       "\n",
       "                                                        bbh_cot_fewshot_geometric_shapes  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - bbh_cot_fewshot_geometric_shapes...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - bbh_cot_fewshot_geometric_shapes...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - bbh_cot_fewshot_geometric_shapes...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - bbh_cot_fewshot_geometric_shapes...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - bbh_cot_fewshot_geometric_shapes...   \n",
       "\n",
       "                                       ...  \\\n",
       "name                                   ...   \n",
       "allenai/Llama-3.1-Tulu-3-8B            ...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        ...   \n",
       "marin-community/marin-8b-instruct      ...   \n",
       "mistralai/Mistral-Small-Instruct-2409  ...   \n",
       "ibm-granite/granite-3.2-8b-instruct    ...   \n",
       "\n",
       "                                                                      mmlu_pro_economics  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - economics', 'exact_match,custom-...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - economics', 'exact_match,custom-...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - economics', 'exact_match,custom-...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - economics', 'exact_match,custom-...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - economics', 'exact_match,custom-...   \n",
       "\n",
       "                                                                    mmlu_pro_engineering  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - engineering', 'exact_match,custo...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - engineering', 'exact_match,custo...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - engineering', 'exact_match,custo...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - engineering', 'exact_match,custo...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - engineering', 'exact_match,custo...   \n",
       "\n",
       "                                                                         mmlu_pro_health  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - health', 'exact_match,custom-ext...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - health', 'exact_match,custom-ext...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - health', 'exact_match,custom-ext...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - health', 'exact_match,custom-ext...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - health', 'exact_match,custom-ext...   \n",
       "\n",
       "                                                                        mmlu_pro_history  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - history', 'exact_match,custom-ex...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - history', 'exact_match,custom-ex...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - history', 'exact_match,custom-ex...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - history', 'exact_match,custom-ex...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - history', 'exact_match,custom-ex...   \n",
       "\n",
       "                                                                            mmlu_pro_law  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - law', 'exact_match,custom-extrac...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - law', 'exact_match,custom-extrac...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - law', 'exact_match,custom-extrac...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - law', 'exact_match,custom-extrac...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - law', 'exact_match,custom-extrac...   \n",
       "\n",
       "                                                                           mmlu_pro_math  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - math', 'exact_match,custom-extra...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - math', 'exact_match,custom-extra...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - math', 'exact_match,custom-extra...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - math', 'exact_match,custom-extra...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - math', 'exact_match,custom-extra...   \n",
       "\n",
       "                                                                          mmlu_pro_other  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - other', 'exact_match,custom-extr...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - other', 'exact_match,custom-extr...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - other', 'exact_match,custom-extr...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - other', 'exact_match,custom-extr...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - other', 'exact_match,custom-extr...   \n",
       "\n",
       "                                                                     mmlu_pro_philosophy  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - philosophy', 'exact_match,custom...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - philosophy', 'exact_match,custom...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - philosophy', 'exact_match,custom...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - philosophy', 'exact_match,custom...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - philosophy', 'exact_match,custom...   \n",
       "\n",
       "                                                                        mmlu_pro_physics  \\\n",
       "name                                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - physics', 'exact_match,custom-ex...   \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - physics', 'exact_match,custom-ex...   \n",
       "marin-community/marin-8b-instruct      {'alias': ' - physics', 'exact_match,custom-ex...   \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - physics', 'exact_match,custom-ex...   \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - physics', 'exact_match,custom-ex...   \n",
       "\n",
       "                                                                     mmlu_pro_psychology  \n",
       "name                                                                                      \n",
       "allenai/Llama-3.1-Tulu-3-8B            {'alias': ' - psychology', 'exact_match,custom...  \n",
       "allenai/OLMo-2-1124-7B-Instruct        {'alias': ' - psychology', 'exact_match,custom...  \n",
       "marin-community/marin-8b-instruct      {'alias': ' - psychology', 'exact_match,custom...  \n",
       "mistralai/Mistral-Small-Instruct-2409  {'alias': ' - psychology', 'exact_match,custom...  \n",
       "ibm-granite/granite-3.2-8b-instruct    {'alias': ' - psychology', 'exact_match,custom...  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "folder_name = \"results\"\n",
    "pardir = Path(__name__).parent\n",
    "folder = os.path.join(pardir, folder_name)\n",
    "models = [f for f in Path(folder).iterdir() if f.is_dir()]\n",
    "contents = []\n",
    "for model in models:\n",
    "    files = [f for f in Path(model).iterdir() if f.is_file()]\n",
    "    assert len(files) == 1, \"should only have 1 result\"\n",
    "    file = files[0]\n",
    "    with open(file, \"r\") as f:\n",
    "        loaded = json.load(f)\n",
    "        name = loaded['configs']['arc_challenge']['metadata']['pretrained']\n",
    "        results = loaded['results']\n",
    "        results['name'] = name\n",
    "        contents.append(results)\n",
    "df = pd.DataFrame(contents)\n",
    "df=df.set_index(\"name\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_try = [\"acc,none\", # default others\n",
    "\"prompt_level_strict_acc,none\", #ifeval\n",
    "\"pass@1,create_test\", #humaneval\n",
    "\"pass_at_1,none\", #mbpp\n",
    "\"exact_match,custom-extract\",\n",
    "\"exact_match,get-answer\"] #default others\n",
    "def extract(series): #expects you to do x['col']\n",
    "    data = []\n",
    "    for row in series:\n",
    "        keys = list(row.keys())\n",
    "        # import pdb; pdb.set_trace()\n",
    "        for key in keys_to_try:\n",
    "            if key in keys:\n",
    "                data.append(row[key])\n",
    "    return pd.Series(data, index = series.index)\n",
    "\n",
    "df = df.apply(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arc_challenge</th>\n",
       "      <th>arc_easy</th>\n",
       "      <th>bbh</th>\n",
       "      <th>ifeval</th>\n",
       "      <th>humaneval</th>\n",
       "      <th>mbpp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allenai/Llama-3.1-Tulu-3-8B</th>\n",
       "      <td>0.565700</td>\n",
       "      <td>0.839226</td>\n",
       "      <td>0.689141</td>\n",
       "      <td>0.593346</td>\n",
       "      <td>0.628049</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allenai/OLMo-2-1124-7B-Instruct</th>\n",
       "      <td>0.551195</td>\n",
       "      <td>0.835438</td>\n",
       "      <td>0.490401</td>\n",
       "      <td>0.591497</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marin-community/marin-8b-instruct</th>\n",
       "      <td>0.530717</td>\n",
       "      <td>0.815236</td>\n",
       "      <td>0.264322</td>\n",
       "      <td>0.497227</td>\n",
       "      <td>0.573171</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistralai/Mistral-Small-Instruct-2409</th>\n",
       "      <td>0.589590</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.771617</td>\n",
       "      <td>0.574861</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ibm-granite/granite-3.2-8b-instruct</th>\n",
       "      <td>0.513652</td>\n",
       "      <td>0.801768</td>\n",
       "      <td>0.711411</td>\n",
       "      <td>0.628466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       arc_challenge  arc_easy       bbh  \\\n",
       "name                                                                       \n",
       "allenai/Llama-3.1-Tulu-3-8B                 0.565700  0.839226  0.689141   \n",
       "allenai/OLMo-2-1124-7B-Instruct             0.551195  0.835438  0.490401   \n",
       "marin-community/marin-8b-instruct           0.530717  0.815236  0.264322   \n",
       "mistralai/Mistral-Small-Instruct-2409       0.589590  0.847222  0.771617   \n",
       "ibm-granite/granite-3.2-8b-instruct         0.513652  0.801768  0.711411   \n",
       "\n",
       "                                         ifeval  humaneval   mbpp  \n",
       "name                                                               \n",
       "allenai/Llama-3.1-Tulu-3-8B            0.593346   0.628049  0.006  \n",
       "allenai/OLMo-2-1124-7B-Instruct        0.591497   0.414634  0.278  \n",
       "marin-community/marin-8b-instruct      0.497227   0.573171  0.024  \n",
       "mistralai/Mistral-Small-Instruct-2409  0.574861   0.652439  0.570  \n",
       "ibm-granite/granite-3.2-8b-instruct    0.628466   0.000000  0.540  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_we_care_abt = ['arc_challenge', 'arc_easy', 'bbh', 'ifeval', 'humaneval', 'mbpp']\n",
    "df[columns_we_care_abt]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bfcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
