{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facet Interaction Mini-Benchmark\n",
    "\n",
    "Plan and run a tiny, fast-to-run probe across four facets — reasoning, instruction-following, coding, and agentic tool-use. Evaluate a few LLMs on a handful of items per facet, then fit a 2D tensor-product spline to visualize how pairs of facet scores relate (as hills/troughs) in 3D. The notebook is self-contained and uses small prompts so it can run quickly on CPU.\n",
    "\n",
    "What you get:\n",
    "- Minimal benchmark slices for each facet (handful of curated prompts)\n",
    "- Small suite of models (defaults to lightweight; toggle heavier ones)\n",
    "- Automated scoring heuristics\n",
    "- Spline fitting and 3D plotting for pairwise facet relationships\n",
    "- Hooks to extend with your own prompts/models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight dependency check (SciPy for tensor-product splines)\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if importlib.util.find_spec(\"scipy\") is None:\n",
    "    print(\"Installing scipy for spline fitting...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"scipy\"])\n",
    "else:\n",
    "    print(\"SciPy already available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "from openai import OpenAI\n",
    "\n",
    "# Notebook-level settings\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "OPENROUTER_BASE = \"https://openrouter.ai/api/v1\"\n",
    "OPENROUTER_KEY = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_KEY:\n",
    "    print(\"WARNING: set OPENROUTER_API_KEY in your environment for live calls.\")\n",
    "\n",
    "client = OpenAI(base_url=OPENROUTER_BASE, api_key=OPENROUTER_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-benchmark slices\n",
    "Each facet uses 3–4 small prompts to keep runtime low. Scoring is heuristic/regex-based to avoid heavy post-processing. Feel free to replace with your own tasks or plug in richer evals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Task:\n",
    "    facet: str\n",
    "    name: str\n",
    "    prompt: str\n",
    "    expected: Dict\n",
    "    scorer: Callable[[str, Dict], float]\n",
    "\n",
    "\n",
    "def numeric_in_text(text: str) -> List[int]:\n",
    "    return [int(x) for x in re.findall(r\"-?\\d+\", text)]\n",
    "\n",
    "\n",
    "def score_reasoning(output: str, expected: Dict) -> float:\n",
    "    nums = numeric_in_text(output)\n",
    "    return 1.0 if expected.get(\"answer\") in nums else 0.0\n",
    "\n",
    "\n",
    "def score_instruction(output: str, expected: Dict) -> float:\n",
    "    must_include = expected.get(\"must_include\", [])\n",
    "    for token in must_include:\n",
    "        if token.lower() not in output.lower():\n",
    "            return 0.0\n",
    "    if expected.get(\"format\") == \"json\":\n",
    "        try:\n",
    "            json.loads(output)\n",
    "        except json.JSONDecodeError:\n",
    "            return 0.0\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "def score_coding(output: str, expected: Dict) -> float:\n",
    "    patterns = expected.get(\"patterns\", [])\n",
    "    if patterns and not all(re.search(pat, output, flags=re.IGNORECASE) for pat in patterns):\n",
    "        return 0.0\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "def score_tool(output: str, expected: Dict) -> float:\n",
    "    required = expected.get(\"required\", [])\n",
    "    for token in required:\n",
    "        if token.lower() not in output.lower():\n",
    "            return 0.0\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "TASKS: List[Task] = [\n",
    "    # Reasoning\n",
    "    Task(\n",
    "        facet=\"reasoning\",\n",
    "        name=\"math_apples\",\n",
    "        prompt=\"Sarah has 3 apples. She buys 4 more and eats 2. How many apples remain?\",\n",
    "        expected={\"answer\": 5},\n",
    "        scorer=score_reasoning,\n",
    "    ),\n",
    "    Task(\n",
    "        facet=\"reasoning\",\n",
    "        name=\"train_time\",\n",
    "        prompt=\"A train travels 60 miles in 1.5 hours. What is its average speed in mph?\",\n",
    "        expected={\"answer\": 40},\n",
    "        scorer=score_reasoning,\n",
    "    ),\n",
    "    Task(\n",
    "        facet=\"reasoning\",\n",
    "        name=\"ratio_mix\",\n",
    "        prompt=\"You mix 2 parts red paint with 3 parts blue. If you use 10 cups total, how many cups are blue?\",\n",
    "        expected={\"answer\": 6},\n",
    "        scorer=score_reasoning,\n",
    "    ),\n",
    "    # Instruction following\n",
    "    Task(\n",
    "        facet=\"instruction\",\n",
    "        name=\"csv_format\",\n",
    "        prompt=\"Return the three words 'oak', 'pine', 'birch' as a single comma-separated line (no spaces).\",\n",
    "        expected={\"must_include\": [\"oak,pine,birch\"]},\n",
    "        scorer=score_instruction,\n",
    "    ),\n",
    "    Task(\n",
    "        facet=\"instruction\",\n",
    "        name=\"json_shape\",\n",
    "        prompt=\"Respond with JSON containing keys name and priority for the task 'backup data' with high priority.\",\n",
    "        expected={\"must_include\": [\"backup\", \"high\"], \"format\": \"json\"},\n",
    "        scorer=score_instruction,\n",
    "    ),\n",
    "    Task(\n",
    "        facet=\"instruction\",\n",
    "        name=\"style_rule\",\n",
    "        prompt=\"Answer 'ready' in ALL CAPS followed by an exclamation mark, nothing else.\",\n",
    "        expected={\"must_include\": [\"READY!\"]},\n",
    "        scorer=score_instruction,\n",
    "    ),\n",
    "    # Coding (format/intent, not runtime exec for safety)\n",
    "    Task(\n",
    "        facet=\"coding\",\n",
    "        name=\"add_func\",\n",
    "        prompt=\"Write a Python function named add_numbers(a, b) that returns their sum.\",\n",
    "        expected={\"patterns\": [r\"def\\s+add_numbers\", r\"return\\s+a\\s*\\+\\s*b\"]},\n",
    "        scorer=score_coding,\n",
    "    ),\n",
    "    Task(\n",
    "        facet=\"coding\",\n",
    "        name=\"list_comprehension\",\n",
    "        prompt=\"Give a one-line Python list comprehension that squares numbers 1 through 4.\",\n",
    "        expected={\"patterns\": [r\"\\[n\\*\\*2\\s+for\\s+n\\s+in\\s+range\\(1,\\s*5\\)\"]},\n",
    "        scorer=score_coding,\n",
    "    ),\n",
    "    Task(\n",
    "        facet=\"coding\",\n",
    "        name=\"doc_comment\",\n",
    "        prompt=\"Provide a short Python function with a docstring that reverses a string.\",\n",
    "        expected={\"patterns\": [r\"def\", r\"docstring\", r\"[::-1]\"]},\n",
    "        scorer=score_coding,\n",
    "    ),\n",
    "    # Agentic tool-use (checks for structured tool calls)\n",
    "    Task(\n",
    "        facet=\"tool_use\",\n",
    "        name=\"search_tool\",\n",
    "        prompt=\"Use a SEARCH tool to look up 'weather in Paris' and return the action line only.\",\n",
    "        expected={\"required\": [\"SEARCH\", \"Paris\"]},\n",
    "        scorer=score_tool,\n",
    "    ),\n",
    "    Task(\n",
    "        facet=\"tool_use\",\n",
    "        name=\"calc_tool\",\n",
    "        prompt=\"Call a CALC tool to compute 12 * 7. Return only the tool call line.\",\n",
    "        expected={\"required\": [\"CALC\", \"12\", \"7\"]},\n",
    "        scorer=score_tool,\n",
    "    ),\n",
    "    Task(\n",
    "        facet=\"tool_use\",\n",
    "        name=\"retrieval_tool\",\n",
    "        prompt=\"Trigger a RETRIEVE tool for the topic 'quantum tunneling'. Output only the tool command.\",\n",
    "        expected={\"required\": [\"RETRIEVE\", \"quantum\"]},\n",
    "        scorer=score_tool,\n",
    "    ),\n",
    "]\n",
    "\n",
    "FACETS = [\"reasoning\", \"instruction\", \"coding\", \"tool_use\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to probe (OpenRouter)\n",
    "Toggle `enabled` to include/exclude. Defaults favor inexpensive/small OpenRouter models; swap in your choices. Set `OPENROUTER_API_KEY` before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SPECS = [\n",
    "    {\n",
    "        \"id\": \"mistralai/mistral-small-latest\",  # adjust to your available OpenRouter models\n",
    "        \"alias\": \"mistral-small\",\n",
    "        \"enabled\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"qwen/qwen-2.5-3b-instruct\",  # swap to another inexpensive model if unavailable\n",
    "        \"alias\": \"qwen2.5-3b\",\n",
    "        \"enabled\": False,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"anthropic/claude-3.5-haiku\",\n",
    "        \"alias\": \"claude-3.5-haiku\",\n",
    "        \"enabled\": False,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def chat_with_model(model_id: str, prompt: str, max_tokens: int = 128, temperature: float = 0.1) -> str:\n",
    "    \"\"\"Single OpenRouter chat completion with light retry.\"\"\"\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            return resp.choices[0].message.content.strip()\n",
    "        except Exception as exc:  # pragma: no cover - network path\n",
    "            last_err = exc\n",
    "            wait = 1.5 * (attempt + 1)\n",
    "            print(f\"Retry {attempt + 1}/3 after error: {exc} (waiting {wait:.1f}s)\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"Failed OpenRouter call after retries: {last_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation harness\n",
    "Runs each model on the prompt set, captures generations, applies facet-specific scoring, and aggregates scores per facet and per model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_KW = {\"max_tokens\": 128, \"temperature\": 0.2}\n",
    "\n",
    "\n",
    "def run_model_on_tasks(model_id: str, tasks: List[Task]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for task in tasks:\n",
    "        tail = chat_with_model(model_id, task.prompt, **GENERATION_KW)\n",
    "        score = task.scorer(tail, task.expected)\n",
    "        rows.append({\"facet\": task.facet, \"task\": task.name, \"prompt\": task.prompt, \"response\": tail, \"score\": score})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def evaluate_models(model_specs: List[Dict]) -> pd.DataFrame:\n",
    "    all_rows = []\n",
    "    for spec in model_specs:\n",
    "        if not spec.get(\"enabled\", True):\n",
    "            continue\n",
    "        print(f\"Running {spec['alias']} via OpenRouter ...\")\n",
    "        df = run_model_on_tasks(spec[\"id\"], TASKS)\n",
    "        df[\"model\"] = spec[\"alias\"]\n",
    "        all_rows.append(df)\n",
    "    return pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame()\n",
    "\n",
    "\n",
    "RUN_EVAL = True  # set False to skip generation and load cached results\n",
    "results_df = None\n",
    "\n",
    "if RUN_EVAL:\n",
    "    if not OPENROUTER_KEY:\n",
    "        raise EnvironmentError(\"OPENROUTER_API_KEY not set; cannot run live evals.\")\n",
    "    start = time.time()\n",
    "    results_df = evaluate_models(MODEL_SPECS)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Ran {len(results_df)} model-task pairs in {elapsed:.1f}s\")\n",
    "else:\n",
    "    print(\"Generation skipped; populate results_df manually or load external evals.\")\n",
    "\n",
    "results_df.head() if results_df is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate scores\n",
    "Collapse to per-model/per-facet means plus a wide table for plotting. Optionally merge precomputed evals (LM Eval outputs or UKAI safety eval exports) before plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally load external evals (LM Eval outputs, UKAI safety eval exports) into\n",
    "# a DataFrame with columns: model, facet, score. Append them to results_df below.\n",
    "EXTERNAL_EVAL_PATHS: List[Path] = []  # e.g., [Path(\"/path/to/ukai_safety_eval.csv\")]\n",
    "\n",
    "external_rows = []\n",
    "for p in EXTERNAL_EVAL_PATHS:\n",
    "    if not p.exists():\n",
    "        print(f\"External eval not found: {p}\")\n",
    "        continue\n",
    "    ext_df = pd.read_csv(p)\n",
    "    if not {\"model\", \"facet\", \"score\"}.issubset(ext_df.columns):\n",
    "        raise ValueError(f\"External eval file {p} missing required columns model/facet/score\")\n",
    "    external_rows.append(ext_df)\n",
    "\n",
    "if external_rows:\n",
    "    external_df = pd.concat(external_rows, ignore_index=True)\n",
    "    if results_df is None:\n",
    "        results_df = external_df\n",
    "    else:\n",
    "        results_df = pd.concat([results_df, external_df], ignore_index=True)\n",
    "\n",
    "if results_df is None or results_df.empty:\n",
    "    raise ValueError(\"No results to aggregate. Run cells above or load external evals.\")\n",
    "\n",
    "facet_means = results_df.groupby([\"model\", \"facet\"]).agg(score=(\"score\", \"mean\")).reset_index()\n",
    "wide = facet_means.pivot(index=\"model\", columns=\"facet\", values=\"score\").reset_index().fillna(0.0)\n",
    "\n",
    "print(\"Facet means:\\n\", facet_means)\n",
    "print(\"\\nWide table:\\n\", wide)\n",
    "\n",
    "wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor-product spline & 3D plots\n",
    "Fits a smooth bivariate spline over two facet axes to predict a third metric (or any target column). Uses `SmoothBivariateSpline` when possible and falls back to radial basis interpolation if the sample is too small or degenerate. Red dots show the observed model points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import Rbf, SmoothBivariateSpline\n",
    "\n",
    "\n",
    "def fit_surface(df: pd.DataFrame, x_col: str, y_col: str, z_col: str, grid_n: int = 40):\n",
    "    x = df[x_col].to_numpy()\n",
    "    y = df[y_col].to_numpy()\n",
    "    z = df[z_col].to_numpy()\n",
    "\n",
    "    # Build grid for plotting\n",
    "    xg = np.linspace(x.min(), x.max(), grid_n)\n",
    "    yg = np.linspace(y.min(), y.max(), grid_n)\n",
    "    Xg, Yg = np.meshgrid(xg, yg)\n",
    "\n",
    "    try:\n",
    "        spline = SmoothBivariateSpline(x, y, z, kx=3, ky=3, s=0.1)\n",
    "        Zg = spline(xg, yg)\n",
    "        method = \"SmoothBivariateSpline\"\n",
    "    except Exception as exc:  # fallback for tiny sample sizes\n",
    "        rbf = Rbf(x, y, z, function=\"thin_plate\")\n",
    "        Zg = rbf(Xg, Yg)\n",
    "        method = f\"RBF fallback ({exc.__class__.__name__})\"\n",
    "    return Xg, Yg, Zg, method\n",
    "\n",
    "\n",
    "def plot_surface(df: pd.DataFrame, x_col: str, y_col: str, z_col: str, title: str):\n",
    "    Xg, Yg, Zg, method = fit_surface(df, x_col, y_col, z_col)\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    surf = ax.plot_surface(Xg, Yg, Zg, cmap=cm.viridis, alpha=0.8)\n",
    "    ax.scatter(df[x_col], df[y_col], df[z_col], color=\"red\", s=60, label=\"models\")\n",
    "    ax.set_xlabel(x_col)\n",
    "    ax.set_ylabel(y_col)\n",
    "    ax.set_zlabel(z_col)\n",
    "    ax.set_title(f\"{title}\\n{method}\")\n",
    "    fig.colorbar(surf, shrink=0.6, aspect=12)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define which surfaces to visualize (x, y -> z)\n",
    "SURFACES = [\n",
    "    (\"reasoning\", \"instruction\", \"coding\"),\n",
    "    (\"reasoning\", \"coding\", \"tool_use\"),\n",
    "    (\"instruction\", \"tool_use\", \"reasoning\"),\n",
    "]\n",
    "\n",
    "for x_col, y_col, z_col in SURFACES:\n",
    "    if not set([x_col, y_col, z_col]).issubset(set(wide.columns)):\n",
    "        print(f\"Missing columns for {x_col}/{y_col}/{z_col}; skip.\")\n",
    "        continue\n",
    "    plot_surface(wide, x_col, y_col, z_col, title=f\"{x_col} vs {y_col} -> {z_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips to extend\n",
    "- Add prompts to `TASKS` or adjust scoring heuristics per facet.\n",
    "- Toggle/expand `MODEL_SPECS` to probe more OpenRouter models; keep `max_tokens` small for cost/latency.\n",
    "- Swap the `SURFACES` tuples to explore other relationships or use `wide.eval()` to craft composite targets.\n",
    "- If you already have scores (LM Eval or UKAI safety exports), set `RUN_EVAL = False` and list them in `EXTERNAL_EVAL_PATHS` to avoid regenerating text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
