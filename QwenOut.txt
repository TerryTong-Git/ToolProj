INFO 11-22 22:27:37 [__init__.py:256] Automatically detected platform cuda.
WARNING 11-22 22:27:43 [config.py:2599] Casting torch.bfloat16 to torch.float16.
INFO 11-22 22:27:56 [config.py:583] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 11-22 22:27:56 [config.py:1515] Defaulting to use mp for distributed inference
INFO 11-22 22:27:56 [config.py:1693] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 11-22 22:27:59 [core.py:53] Initializing a V1 LLM engine (v0.8.0) with config: model='Qwen/Qwen2.5-32B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-32B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir='/nlpgpu/data/terry/ToolProj/src/models', load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-32B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 11-22 22:27:59 [multiproc_worker_utils.py:310] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 11-22 22:28:00 [custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 11-22 22:28:00 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 10485760, 10, 'psm_949f0e0c'), local_subscribe_addr='ipc:///tmp/8b87643f-2296-4681-aa5e-bfa3ce8bfa9b', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 11-22 22:28:02 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f12726968c0>
[1;36m(VllmWorker rank=0 pid=598)[0;0m INFO 11-22 22:28:02 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f88e6a19'), local_subscribe_addr='ipc:///tmp/4dbd7dd3-174b-41f4-b9a9-5380e8b18dcc', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 11-22 22:28:03 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1272696ef0>
[1;36m(VllmWorker rank=1 pid=629)[0;0m INFO 11-22 22:28:03 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_00510569'), local_subscribe_addr='ipc:///tmp/20720992-3e16-4c44-a57b-4c407c046fb0', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 11-22 22:28:04 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f12726968c0>
[1;36m(VllmWorker rank=2 pid=683)[0;0m INFO 11-22 22:28:04 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9ee71de5'), local_subscribe_addr='ipc:///tmp/06a9eacb-1197-4550-9168-b3cddce781d9', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 11-22 22:28:05 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1272696e60>
[1;36m(VllmWorker rank=3 pid=721)[0;0m INFO 11-22 22:28:05 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3ce61433'), local_subscribe_addr='ipc:///tmp/bb519d5e-5be3-4a0a-b73f-ba52adc6710f', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 11-22 22:28:07 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f12726950f0>
[1;36m(VllmWorker rank=4 pid=756)[0;0m INFO 11-22 22:28:07 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_70a365ab'), local_subscribe_addr='ipc:///tmp/75a2a7df-6443-4d3a-9b7d-befd909f1cd9', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 11-22 22:28:08 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1272694df0>
[1;36m(VllmWorker rank=5 pid=783)[0;0m INFO 11-22 22:28:08 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_25131c93'), local_subscribe_addr='ipc:///tmp/529c8610-4c47-4fac-b1db-b010e31a7424', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 11-22 22:28:09 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1272696ad0>
[1;36m(VllmWorker rank=6 pid=810)[0;0m INFO 11-22 22:28:09 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8372761b'), local_subscribe_addr='ipc:///tmp/1c504ce6-8c8c-41d3-8b1f-150e36af39de', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 11-22 22:28:10 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1272697be0>
[1;36m(VllmWorker rank=7 pid=838)[0;0m INFO 11-22 22:28:10 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1b32d6a2'), local_subscribe_addr='ipc:///tmp/dab16ccf-2b43-4133-96bc-b7e2e7dd86af', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=629)[0;0m INFO 11-22 22:28:11 [utils.py:925] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=629)[0;0m INFO 11-22 22:28:11 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=2 pid=683)[0;0m INFO 11-22 22:28:11 [utils.py:925] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=598)[0;0m INFO 11-22 22:28:11 [utils.py:925] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=683)[0;0m INFO 11-22 22:28:11 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=0 pid=598)[0;0m INFO 11-22 22:28:11 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=3 pid=721)[0;0m INFO 11-22 22:28:11 [utils.py:925] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=721)[0;0m INFO 11-22 22:28:11 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=7 pid=838)[0;0m INFO 11-22 22:28:11 [utils.py:925] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=7 pid=838)[0;0m INFO 11-22 22:28:11 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=6 pid=810)[0;0m INFO 11-22 22:28:11 [utils.py:925] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=6 pid=810)[0;0m INFO 11-22 22:28:11 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=5 pid=783)[0;0m INFO 11-22 22:28:11 [utils.py:925] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=5 pid=783)[0;0m INFO 11-22 22:28:11 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=4 pid=756)[0;0m INFO 11-22 22:28:11 [utils.py:925] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=4 pid=756)[0;0m INFO 11-22 22:28:11 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=3 pid=721)[0;0m WARNING 11-22 22:28:14 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=598)[0;0m WARNING 11-22 22:28:14 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=7 pid=838)[0;0m WARNING 11-22 22:28:14 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=5 pid=783)[0;0m WARNING 11-22 22:28:14 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=1 pid=629)[0;0m WARNING 11-22 22:28:14 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=6 pid=810)[0;0m WARNING 11-22 22:28:14 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=4 pid=756)[0;0m WARNING 11-22 22:28:14 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=2 pid=683)[0;0m WARNING 11-22 22:28:14 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=598)[0;0m INFO 11-22 22:28:14 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_3cc563eb'), local_subscribe_addr='ipc:///tmp/0c1a37f9-57a3-4606-83c8-0d6d5ca7df0b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=629)[0;0m INFO 11-22 22:28:14 [parallel_state.py:967] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1
[1;36m(VllmWorker rank=2 pid=683)[0;0m INFO 11-22 22:28:14 [parallel_state.py:967] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2
[1;36m(VllmWorker rank=6 pid=810)[0;0m INFO 11-22 22:28:14 [parallel_state.py:967] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6
[1;36m(VllmWorker rank=0 pid=598)[0;0m INFO 11-22 22:28:14 [parallel_state.py:967] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0
[1;36m(VllmWorker rank=3 pid=721)[0;0m INFO 11-22 22:28:14 [parallel_state.py:967] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3
[1;36m(VllmWorker rank=7 pid=838)[0;0m INFO 11-22 22:28:14 [parallel_state.py:967] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7
[1;36m(VllmWorker rank=4 pid=756)[0;0m INFO 11-22 22:28:14 [parallel_state.py:967] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4
[1;36m(VllmWorker rank=5 pid=783)[0;0m INFO 11-22 22:28:14 [parallel_state.py:967] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5
[1;36m(VllmWorker rank=0 pid=598)[0;0m INFO 11-22 22:28:14 [cuda.py:215] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=721)[0;0m INFO 11-22 22:28:14 [cuda.py:215] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=6 pid=810)[0;0m INFO 11-22 22:28:14 [cuda.py:215] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=683)[0;0m INFO 11-22 22:28:14 [cuda.py:215] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=629)[0;0m INFO 11-22 22:28:14 [cuda.py:215] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=5 pid=783)[0;0m INFO 11-22 22:28:14 [cuda.py:215] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=7 pid=838)[0;0m INFO 11-22 22:28:14 [cuda.py:215] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=5 pid=783)[0;0m INFO 11-22 22:28:14 [gpu_model_runner.py:1128] Starting to load model Qwen/Qwen2.5-32B-Instruct...
[1;36m(VllmWorker rank=6 pid=810)[0;0m INFO 11-22 22:28:14 [gpu_model_runner.py:1128] Starting to load model Qwen/Qwen2.5-32B-Instruct...
[1;36m(VllmWorker rank=0 pid=598)[0;0m INFO 11-22 22:28:14 [gpu_model_runner.py:1128] Starting to load model Qwen/Qwen2.5-32B-Instruct...
[1;36m(VllmWorker rank=7 pid=838)[0;0m INFO 11-22 22:28:14 [gpu_model_runner.py:1128] Starting to load model Qwen/Qwen2.5-32B-Instruct...
[1;36m(VllmWorker rank=4 pid=756)[0;0m INFO 11-22 22:28:14 [cuda.py:215] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=683)[0;0m INFO 11-22 22:28:14 [gpu_model_runner.py:1128] Starting to load model Qwen/Qwen2.5-32B-Instruct...
[1;36m(VllmWorker rank=1 pid=629)[0;0m INFO 11-22 22:28:14 [gpu_model_runner.py:1128] Starting to load model Qwen/Qwen2.5-32B-Instruct...
[1;36m(VllmWorker rank=3 pid=721)[0;0m INFO 11-22 22:28:14 [gpu_model_runner.py:1128] Starting to load model Qwen/Qwen2.5-32B-Instruct...
[1;36m(VllmWorker rank=4 pid=756)[0;0m INFO 11-22 22:28:14 [gpu_model_runner.py:1128] Starting to load model Qwen/Qwen2.5-32B-Instruct...
[1;36m(VllmWorker rank=7 pid=838)[0;0m WARNING 11-22 22:28:16 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=2 pid=683)[0;0m WARNING 11-22 22:28:16 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=598)[0;0m WARNING 11-22 22:28:16 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=4 pid=756)[0;0m WARNING 11-22 22:28:16 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=721)[0;0m WARNING 11-22 22:28:16 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=629)[0;0m WARNING 11-22 22:28:16 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=6 pid=810)[0;0m WARNING 11-22 22:28:16 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=5 pid=783)[0;0m WARNING 11-22 22:28:16 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=598)[0;0m INFO 11-22 22:28:16 [weight_utils.py:257] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=7 pid=838)[0;0m INFO 11-22 22:28:16 [weight_utils.py:257] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=2 pid=683)[0;0m INFO 11-22 22:28:16 [weight_utils.py:257] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=3 pid=721)[0;0m INFO 11-22 22:28:16 [weight_utils.py:257] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=4 pid=756)[0;0m INFO 11-22 22:28:16 [weight_utils.py:257] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=6 pid=810)[0;0m INFO 11-22 22:28:16 [weight_utils.py:257] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=1 pid=629)[0;0m INFO 11-22 22:28:16 [weight_utils.py:257] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=5 pid=783)[0;0m INFO 11-22 22:28:16 [weight_utils.py:257] Using model weights format ['*.safetensors']
  [2m2025-11-23T03:31:21.170784Z[0m [33m WARN[0m  [33mReqwest(reqwest::Error { kind: Request, url: "https://transfer.xethub.hf.co/xorbs/default/755a48e6cd0123036ec0117bf0d8344fa3bbd74209bde22aca114a793eda633b?X-Xet-Signed-Range=bytes%3D0-63063895&X-Xet-Session-Id=01KAQC2MWWKJXFKRBH8TBJJH4Y&Expires=1763872097&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly90cmFuc2Zlci54ZXRodWIuaGYuY28veG9yYnMvZGVmYXVsdC83NTVhNDhlNmNkMDEyMzAzNmVjMDExN2JmMGQ4MzQ0ZmEzYmJkNzQyMDliZGUyMmFjYTExNGE3OTNlZGE2MzNiP1gtWGV0LVNpZ25lZC1SYW5nZT1ieXRlcyUzRDAtNjMwNjM4OTUmWC1YZXQtU2Vzc2lvbi1JZD0wMUtBUUMyTVdXS0pYRktSQkg4VEJKSkg0WSIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2Mzg3MjA5N319fV19&Signature=GFKRtz~fS8UDTjSBiGvoOXvJrJ4shXcn5H46VVuXZb6nM6DtZDFs-9Jy6ORwgjmwy1anekCCRg~KcikST-c~AXb7OYy4RjjQRm~DsCAFFBU6pPWV02Fs~aihuTieMoMVCTusM2garvA2PSViOKY~VLS2EZ0cBTazukq2c2xy17jDGWOdqbb9th5c~J2W7~wWn3msOler14GTQT4M6NAuLo-VHaqJsgVtrkmDXWawL0Wa~yme0oIzcTG2gmSq2GJfs2HclVVbEwNDK8RBro-8lq8XakC4fnoh1HlvIUJDslBOXlV14VQMzWZEqaIDxwXaqEisXz2Zf9TU3SyTKbXJKQ__&Key-Pair-Id=K2L8F4GPSG1IFC", source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(IncompleteMessage)) }). Retrying...[0m
    [2;3mat[0m /home/conda/feedstock_root/build_artifacts/bld/rattler-build_hf-xet_1756524959/work/cas_client/src/http_client.rs:226

  [2m2025-11-23T03:31:21.171077Z[0m [33m WARN[0m  [33mRetry attempt #0. Sleeping 2.326095922s before the next attempt[0m
    [2;3mat[0m /home/conda/feedstock_root/build_artifacts/bld/rattler-build_hf-xet_1756524959/build_env/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171

  [2m2025-11-23T03:32:55.583664Z[0m [33m WARN[0m  [33mReqwest(reqwest::Error { kind: Request, url: "https://transfer.xethub.hf.co/xorbs/default/2e405a9c3903087d694f6e0e4f7616a5841cb4f596c626cb8f3e0c448095510d?X-Xet-Signed-Range=bytes%3D0-58169910&X-Xet-Session-Id=01KAQC2MWRERDBK7ZZ7BNEME2A&Expires=1763872302&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly90cmFuc2Zlci54ZXRodWIuaGYuY28veG9yYnMvZGVmYXVsdC8yZTQwNWE5YzM5MDMwODdkNjk0ZjZlMGU0Zjc2MTZhNTg0MWNiNGY1OTZjNjI2Y2I4ZjNlMGM0NDgwOTU1MTBkP1gtWGV0LVNpZ25lZC1SYW5nZT1ieXRlcyUzRDAtNTgxNjk5MTAmWC1YZXQtU2Vzc2lvbi1JZD0wMUtBUUMyTVdSRVJEQks3Wlo3Qk5FTUUyQSIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2Mzg3MjMwMn19fV19&Signature=s-oMxmlvBUl8L0XZKgSYRlCz7OCapfPLHheOBpmhLOer5rxXjP7LbLUP9crDjM005kYuD54-7xpP6jZSFmdgnmZ0flT3RtKlKeohJRh8tN7Azq1adJShyjCX9w6X-v-0I8QTaNKko8jthqNSN~ro7oeKROVuAhBx64i8ZCSd~UzzrxsQo~2ZHEAHkC10-wWN5bOhGb3Mky5UGJzVWBYZx0xza~DTmo6VLurf4zYyHTwvtW1sO8de528NnjJgCThGzl4IwDW-2uteUeS7SCOYq0O4tIk~X~D26W7vOTBtI6qpmsEuzP4itFNwi1GLUqQHZXQYY5qIdLuXlLF4sWwtMw__&Key-Pair-Id=K2L8F4GPSG1IFC", source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(IncompleteMessage)) }). Retrying...[0m
    [2;3mat[0m /home/conda/feedstock_root/build_artifacts/bld/rattler-build_hf-xet_1756524959/work/cas_client/src/http_client.rs:226

  [2m2025-11-23T03:32:55.583978Z[0m [33m WARN[0m  [33mRetry attempt #0. Sleeping 124.676944ms before the next attempt[0m
    [2;3mat[0m /home/conda/feedstock_root/build_artifacts/bld/rattler-build_hf-xet_1756524959/build_env/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171

  [2m2025-11-23T03:32:55.584056Z[0m [33m WARN[0m  [33mReqwest(reqwest::Error { kind: Request, url: "https://transfer.xethub.hf.co/xorbs/default/5851b2eee25c8b6964e99b34d01e7faea40c0837c658047ca7389af3f77b43df?X-Xet-Signed-Range=bytes%3D0-58211606&X-Xet-Session-Id=01KAQC2MTX4QCGHAR5MHH45QKC&Expires=1763872206&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly90cmFuc2Zlci54ZXRodWIuaGYuY28veG9yYnMvZGVmYXVsdC81ODUxYjJlZWUyNWM4YjY5NjRlOTliMzRkMDFlN2ZhZWE0MGMwODM3YzY1ODA0N2NhNzM4OWFmM2Y3N2I0M2RmP1gtWGV0LVNpZ25lZC1SYW5nZT1ieXRlcyUzRDAtNTgyMTE2MDYmWC1YZXQtU2Vzc2lvbi1JZD0wMUtBUUMyTVRYNFFDR0hBUjVNSEg0NVFLQyIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2Mzg3MjIwNn19fV19&Signature=pJnJGwn0EsoiQ4xLuZrEthfiqGUvt277qgThxQrRRle8so8XsRq7tpprQQ6Bere~uwvLBtU2LWOs2YWQQztdScbxSJcJvYrsnTqYP2xMV4ibv2Gvy0YGhbzr0dBww8CBNmkBE59AVYgQo8KtyAFZtUUjBMAxLdORlWo2iZezRbnLquIueEb8MdE4Rsv9v3DdWofeASwgimr3dvLxGDq2q~LodBQiaU59DiI58m5cpTDJTZkjuBEuHhr0ZjE5e7DenlUd5zImHaXxpcsZqsLv7MFbn70dtcsdFUaAKuAcnoCRi8vglZqL-eFE2Bqpu71leFjonWIfhVtSVKlhE40QDA__&Key-Pair-Id=K2L8F4GPSG1IFC", source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(IncompleteMessage)) }). Retrying...[0m
    [2;3mat[0m /home/conda/feedstock_root/build_artifacts/bld/rattler-build_hf-xet_1756524959/work/cas_client/src/http_client.rs:226

  [2m2025-11-23T03:32:55.584069Z[0m [33m WARN[0m  [33mRetry attempt #0. Sleeping 1.920085363s before the next attempt[0m
    [2;3mat[0m /home/conda/feedstock_root/build_artifacts/bld/rattler-build_hf-xet_1756524959/build_env/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171

