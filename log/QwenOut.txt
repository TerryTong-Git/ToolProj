INFO 12-05 18:21:49 [__init__.py:216] Automatically detected platform cuda.
INFO 12-05 18:21:53 [utils.py:328] non-default args: {'download_dir': '/nlpgpu/data/terry/ToolProj/src/models', 'dtype': 'bfloat16', 'seed': 1, 'max_model_len': 8192, 'tensor_parallel_size': 8, 'enable_prefix_caching': True, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-14B-Instruct'}
INFO 12-05 18:22:05 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM
INFO 12-05 18:22:05 [__init__.py:1815] Using max model len 8192
INFO 12-05 18:22:07 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:08 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:08 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir='/nlpgpu/data/terry/ToolProj/src/models', load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=1, served_model_name=Qwen/Qwen2.5-14B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:08 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_08696136'), local_subscribe_addr='ipc:///tmp/be99c3d0-169d-4101-b0ce-48a24689dfec', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ffd26fdb'), local_subscribe_addr='ipc:///tmp/143a7ae5-1ab9-4724-b31d-e428bd166622', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_edd2907a'), local_subscribe_addr='ipc:///tmp/5db115b0-3c7b-4ac4-998f-10403f1144ee', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9eb6e665'), local_subscribe_addr='ipc:///tmp/58dab5ef-e861-4a4e-8ef4-1d0752c6d541', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_471903ee'), local_subscribe_addr='ipc:///tmp/c2d2af3a-9691-4bda-b99c-bd565d2400f4', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c81bc6ac'), local_subscribe_addr='ipc:///tmp/b35e5e03-ae5f-4d3b-8b7e-998c8e201949', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c2e9f17d'), local_subscribe_addr='ipc:///tmp/57faaecb-8873-4fbd-9d46-e0b5437220b8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_89f63bd8'), local_subscribe_addr='ipc:///tmp/da73e815-3620-4909-be22-abf8ffddac51', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f9934cbd'), local_subscribe_addr='ipc:///tmp/8dda7cd4-b14e-49ad-9824-eea8f2779514', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:26 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_e8f0e531'), local_subscribe_addr='ipc:///tmp/9690272b-e922-48bd-8aca-6b7ded23c2f3', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:28 [parallel_state.py:1165] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:28 [parallel_state.py:1165] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:28 [parallel_state.py:1165] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:28 [parallel_state.py:1165] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:28 [parallel_state.py:1165] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:28 [parallel_state.py:1165] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:28 [parallel_state.py:1165] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:22:28 [parallel_state.py:1165] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=62072)[0;0m WARNING 12-05 18:22:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP1 pid=62081)[0;0m INFO 12-05 18:22:29 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP6 pid=62094)[0;0m INFO 12-05 18:22:29 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP4 pid=62088)[0;0m INFO 12-05 18:22:29 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP7 pid=62096)[0;0m INFO 12-05 18:22:29 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP3 pid=62085)[0;0m INFO 12-05 18:22:29 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP0 pid=62079)[0;0m INFO 12-05 18:22:29 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP2 pid=62083)[0;0m INFO 12-05 18:22:29 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP5 pid=62090)[0;0m INFO 12-05 18:22:29 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP4 pid=62088)[0;0m INFO 12-05 18:22:31 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP7 pid=62096)[0;0m INFO 12-05 18:22:31 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP3 pid=62085)[0;0m INFO 12-05 18:22:31 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP6 pid=62094)[0;0m INFO 12-05 18:22:31 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP1 pid=62081)[0;0m INFO 12-05 18:22:31 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP5 pid=62090)[0;0m INFO 12-05 18:22:31 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP0 pid=62079)[0;0m INFO 12-05 18:22:31 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP2 pid=62083)[0;0m INFO 12-05 18:22:31 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP1 pid=62081)[0;0m INFO 12-05 18:22:31 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP3 pid=62085)[0;0m INFO 12-05 18:22:31 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP5 pid=62090)[0;0m INFO 12-05 18:22:31 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP6 pid=62094)[0;0m INFO 12-05 18:22:31 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP7 pid=62096)[0;0m INFO 12-05 18:22:31 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP4 pid=62088)[0;0m INFO 12-05 18:22:31 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP0 pid=62079)[0;0m INFO 12-05 18:22:31 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP2 pid=62083)[0;0m INFO 12-05 18:22:31 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP3 pid=62085)[0;0m INFO 12-05 18:22:33 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP1 pid=62081)[0;0m INFO 12-05 18:22:33 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP0 pid=62079)[0;0m INFO 12-05 18:22:33 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP4 pid=62088)[0;0m INFO 12-05 18:22:33 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP6 pid=62094)[0;0m INFO 12-05 18:22:33 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP5 pid=62090)[0;0m INFO 12-05 18:22:33 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP2 pid=62083)[0;0m INFO 12-05 18:22:33 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP7 pid=62096)[0;0m INFO 12-05 18:22:33 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP2 pid=62083)[0;0m INFO 12-05 18:23:07 [default_loader.py:268] Loading weights took 33.28 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP3 pid=62085)[0;0m INFO 12-05 18:23:07 [default_loader.py:268] Loading weights took 33.84 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP4 pid=62088)[0;0m INFO 12-05 18:23:07 [default_loader.py:268] Loading weights took 34.15 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP1 pid=62081)[0;0m INFO 12-05 18:23:07 [default_loader.py:268] Loading weights took 33.73 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP0 pid=62079)[0;0m INFO 12-05 18:23:07 [default_loader.py:268] Loading weights took 33.90 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP6 pid=62094)[0;0m INFO 12-05 18:23:07 [default_loader.py:268] Loading weights took 34.03 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP5 pid=62090)[0;0m INFO 12-05 18:23:07 [default_loader.py:268] Loading weights took 33.40 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP7 pid=62096)[0;0m INFO 12-05 18:23:08 [default_loader.py:268] Loading weights took 34.00 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP4 pid=62088)[0;0m INFO 12-05 18:23:11 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 36.465040 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP5 pid=62090)[0;0m INFO 12-05 18:23:11 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 36.446524 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP3 pid=62085)[0;0m INFO 12-05 18:23:11 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 36.477997 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP6 pid=62094)[0;0m INFO 12-05 18:23:11 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 36.475410 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP1 pid=62081)[0;0m INFO 12-05 18:23:11 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 36.470019 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP7 pid=62096)[0;0m INFO 12-05 18:23:11 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 36.674539 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP0 pid=62079)[0;0m INFO 12-05 18:23:11 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 36.362220 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP2 pid=62083)[0;0m INFO 12-05 18:23:11 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 36.228239 seconds
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP3 pid=62085)[0;0m INFO 12-05 18:24:17 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_3_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP5 pid=62090)[0;0m INFO 12-05 18:24:17 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_5_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP3 pid=62085)[0;0m INFO 12-05 18:24:17 [backends.py:550] Dynamo bytecode transform time: 64.06 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP4 pid=62088)[0;0m INFO 12-05 18:24:17 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_4_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP5 pid=62090)[0;0m INFO 12-05 18:24:17 [backends.py:550] Dynamo bytecode transform time: 63.97 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP7 pid=62096)[0;0m INFO 12-05 18:24:17 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_7_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP7 pid=62096)[0;0m INFO 12-05 18:24:17 [backends.py:550] Dynamo bytecode transform time: 64.01 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP4 pid=62088)[0;0m INFO 12-05 18:24:17 [backends.py:550] Dynamo bytecode transform time: 64.07 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP0 pid=62079)[0;0m INFO 12-05 18:24:17 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP1 pid=62081)[0;0m INFO 12-05 18:24:17 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_1_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP1 pid=62081)[0;0m INFO 12-05 18:24:17 [backends.py:550] Dynamo bytecode transform time: 64.05 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP0 pid=62079)[0;0m INFO 12-05 18:24:17 [backends.py:550] Dynamo bytecode transform time: 64.04 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP6 pid=62094)[0;0m INFO 12-05 18:24:17 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_6_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP6 pid=62094)[0;0m INFO 12-05 18:24:17 [backends.py:550] Dynamo bytecode transform time: 64.09 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP2 pid=62083)[0;0m INFO 12-05 18:24:17 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_2_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP2 pid=62083)[0;0m INFO 12-05 18:24:17 [backends.py:550] Dynamo bytecode transform time: 64.13 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP0 pid=62079)[0;0m INFO 12-05 18:24:23 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP7 pid=62096)[0;0m INFO 12-05 18:24:25 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP2 pid=62083)[0;0m INFO 12-05 18:24:26 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP1 pid=62081)[0;0m INFO 12-05 18:24:26 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP3 pid=62085)[0;0m INFO 12-05 18:24:26 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP4 pid=62088)[0;0m INFO 12-05 18:24:26 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP5 pid=62090)[0;0m INFO 12-05 18:24:26 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP6 pid=62094)[0;0m INFO 12-05 18:24:26 [backends.py:194] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP0 pid=62079)[0;0m INFO 12-05 18:28:08 [backends.py:215] Compiling a graph for dynamic shape takes 226.64 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP3 pid=62085)[0;0m INFO 12-05 18:28:08 [backends.py:215] Compiling a graph for dynamic shape takes 226.74 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP1 pid=62081)[0;0m INFO 12-05 18:28:09 [backends.py:215] Compiling a graph for dynamic shape takes 226.92 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP6 pid=62094)[0;0m INFO 12-05 18:28:09 [backends.py:215] Compiling a graph for dynamic shape takes 226.96 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP7 pid=62096)[0;0m INFO 12-05 18:28:09 [backends.py:215] Compiling a graph for dynamic shape takes 227.40 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP2 pid=62083)[0;0m INFO 12-05 18:28:09 [backends.py:215] Compiling a graph for dynamic shape takes 227.57 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP4 pid=62088)[0;0m INFO 12-05 18:28:10 [backends.py:215] Compiling a graph for dynamic shape takes 227.86 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP5 pid=62090)[0;0m INFO 12-05 18:28:10 [backends.py:215] Compiling a graph for dynamic shape takes 228.46 s
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP0 pid=62079)[0;0m INFO 12-05 18:28:22 [monitor.py:34] torch.compile takes 290.69 s in total
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP2 pid=62083)[0;0m INFO 12-05 18:28:22 [monitor.py:34] torch.compile takes 291.70 s in total
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP6 pid=62094)[0;0m INFO 12-05 18:28:22 [monitor.py:34] torch.compile takes 291.05 s in total
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP7 pid=62096)[0;0m INFO 12-05 18:28:22 [monitor.py:34] torch.compile takes 291.41 s in total
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP3 pid=62085)[0;0m INFO 12-05 18:28:22 [monitor.py:34] torch.compile takes 290.80 s in total
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP5 pid=62090)[0;0m INFO 12-05 18:28:22 [monitor.py:34] torch.compile takes 292.43 s in total
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP4 pid=62088)[0;0m INFO 12-05 18:28:22 [monitor.py:34] torch.compile takes 291.94 s in total
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP1 pid=62081)[0;0m INFO 12-05 18:28:22 [monitor.py:34] torch.compile takes 290.98 s in total
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP1 pid=62081)[0;0m INFO 12-05 18:28:40 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP4 pid=62088)[0;0m INFO 12-05 18:28:40 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP3 pid=62085)[0;0m INFO 12-05 18:28:40 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP5 pid=62090)[0;0m INFO 12-05 18:28:40 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP2 pid=62083)[0;0m INFO 12-05 18:28:40 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP0 pid=62079)[0;0m INFO 12-05 18:28:40 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP6 pid=62094)[0;0m INFO 12-05 18:28:40 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=62072)[0;0m [1;36m(Worker_TP7 pid=62096)[0;0m INFO 12-05 18:28:40 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:864] GPU KV cache size: 1,643,200 tokens
[1;36m(EngineCore_DP0 pid=62072)[0;0m INFO 12-05 18:28:43 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.59x
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:27 [multiproc_executor.py:149] Worker proc VllmWorker-7 died unexpectedly, shutting down executor.
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 91, in __init__
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 215, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 74, in initialize_from_config
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 259, in collective_rpc
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]     result = get_response(w, dequeue_timeout,
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 239, in get_response
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]     status, result = w.worker_response_mq.dequeue(
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 507, in dequeue
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]     with self.acquire_read(timeout, cancel) as buf:
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]   File "/lcars/home/t/tongt1/.local/share/uv/python/cpython-3.10.18-linux-x86_64-gnu/lib/python3.10/contextlib.py", line 135, in __enter__
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]     return next(self.gen)
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 464, in acquire_read
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718]     raise RuntimeError("cancelled")
[1;36m(EngineCore_DP0 pid=62072)[0;0m ERROR 12-05 18:30:31 [core.py:718] RuntimeError: cancelled
INFO 12-05 18:30:53 [__init__.py:216] Automatically detected platform cuda.
INFO 12-05 18:30:58 [utils.py:328] non-default args: {'download_dir': '/nlpgpu/data/terry/ToolProj/src/models', 'dtype': 'bfloat16', 'seed': 2, 'max_model_len': 8192, 'tensor_parallel_size': 8, 'enable_prefix_caching': True, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-14B-Instruct'}
INFO 12-05 18:31:10 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM
INFO 12-05 18:31:10 [__init__.py:1815] Using max model len 8192
INFO 12-05 18:31:13 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:13 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:13 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir='/nlpgpu/data/terry/ToolProj/src/models', load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=2, served_model_name=Qwen/Qwen2.5-14B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:13 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_48eaed45'), local_subscribe_addr='ipc:///tmp/7cdd0d3a-eb56-48d5-ab37-620a0d426f74', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0929f652'), local_subscribe_addr='ipc:///tmp/2de29f7b-936e-4eec-9bfe-c6564c9dd6fc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4163b682'), local_subscribe_addr='ipc:///tmp/044684d8-a6f1-426f-a6bd-1cf39d939b9a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_57dd9f96'), local_subscribe_addr='ipc:///tmp/86abe480-c2d2-411c-97c7-d41bc022790c', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_44ba7c79'), local_subscribe_addr='ipc:///tmp/540ac3c8-c6dc-4e78-9878-27a39f0ca58b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_69395f40'), local_subscribe_addr='ipc:///tmp/cb747498-b57f-44dd-afe3-0e97f129be75', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_936c8919'), local_subscribe_addr='ipc:///tmp/69152a79-8456-4c34-a1ff-f7262abfe637', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_942ddef9'), local_subscribe_addr='ipc:///tmp/ae27986a-5ba7-40b1-99a7-7b48cb63fd4a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f43b088b'), local_subscribe_addr='ipc:///tmp/9f3b1c1f-fff0-493f-9c19-9e02bb9f8804', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:31 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_45863355'), local_subscribe_addr='ipc:///tmp/f305fac4-743b-44e6-a45a-1d81d9121ee4', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:33 [parallel_state.py:1165] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:33 [parallel_state.py:1165] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:33 [parallel_state.py:1165] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:33 [parallel_state.py:1165] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:33 [parallel_state.py:1165] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:33 [parallel_state.py:1165] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:33 [parallel_state.py:1165] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:31:33 [parallel_state.py:1165] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
[1;36m(EngineCore_DP0 pid=65027)[0;0m WARNING 12-05 18:31:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP4 pid=65041)[0;0m INFO 12-05 18:31:34 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP6 pid=65045)[0;0m INFO 12-05 18:31:34 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP7 pid=65047)[0;0m INFO 12-05 18:31:34 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP1 pid=65035)[0;0m INFO 12-05 18:31:34 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP5 pid=65043)[0;0m INFO 12-05 18:31:34 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP3 pid=65038)[0;0m INFO 12-05 18:31:34 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP2 pid=65037)[0;0m INFO 12-05 18:31:34 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP0 pid=65033)[0;0m INFO 12-05 18:31:34 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP7 pid=65047)[0;0m INFO 12-05 18:31:36 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP6 pid=65045)[0;0m INFO 12-05 18:31:36 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP4 pid=65041)[0;0m INFO 12-05 18:31:36 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP5 pid=65043)[0;0m INFO 12-05 18:31:36 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP3 pid=65038)[0;0m INFO 12-05 18:31:36 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP2 pid=65037)[0;0m INFO 12-05 18:31:36 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP0 pid=65033)[0;0m INFO 12-05 18:31:36 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP1 pid=65035)[0;0m INFO 12-05 18:31:36 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP7 pid=65047)[0;0m INFO 12-05 18:31:36 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP6 pid=65045)[0;0m INFO 12-05 18:31:36 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP4 pid=65041)[0;0m INFO 12-05 18:31:36 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP5 pid=65043)[0;0m INFO 12-05 18:31:36 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP3 pid=65038)[0;0m INFO 12-05 18:31:36 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP2 pid=65037)[0;0m INFO 12-05 18:31:37 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP0 pid=65033)[0;0m INFO 12-05 18:31:37 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP1 pid=65035)[0;0m INFO 12-05 18:31:37 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP1 pid=65035)[0;0m INFO 12-05 18:31:38 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP4 pid=65041)[0;0m INFO 12-05 18:31:38 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP6 pid=65045)[0;0m INFO 12-05 18:31:38 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP5 pid=65043)[0;0m INFO 12-05 18:31:38 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP3 pid=65038)[0;0m INFO 12-05 18:31:38 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP2 pid=65037)[0;0m INFO 12-05 18:31:38 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP7 pid=65047)[0;0m INFO 12-05 18:31:38 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP0 pid=65033)[0;0m INFO 12-05 18:31:38 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP0 pid=65033)[0;0m INFO 12-05 18:36:14 [default_loader.py:268] Loading weights took 275.14 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP1 pid=65035)[0;0m INFO 12-05 18:36:14 [default_loader.py:268] Loading weights took 276.30 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP7 pid=65047)[0;0m INFO 12-05 18:36:14 [default_loader.py:268] Loading weights took 275.60 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP2 pid=65037)[0;0m INFO 12-05 18:36:14 [default_loader.py:268] Loading weights took 275.95 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP6 pid=65045)[0;0m INFO 12-05 18:36:14 [default_loader.py:268] Loading weights took 275.69 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP5 pid=65043)[0;0m INFO 12-05 18:36:14 [default_loader.py:268] Loading weights took 275.55 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP3 pid=65038)[0;0m INFO 12-05 18:36:14 [default_loader.py:268] Loading weights took 275.88 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP4 pid=65041)[0;0m INFO 12-05 18:36:14 [default_loader.py:268] Loading weights took 275.60 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP4 pid=65041)[0;0m INFO 12-05 18:36:18 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 278.270699 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP7 pid=65047)[0;0m INFO 12-05 18:36:18 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 278.145021 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP3 pid=65038)[0;0m INFO 12-05 18:36:18 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 278.113025 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP5 pid=65043)[0;0m INFO 12-05 18:36:18 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 278.128637 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP6 pid=65045)[0;0m INFO 12-05 18:36:18 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 278.113689 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP0 pid=65033)[0;0m INFO 12-05 18:36:18 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 277.962705 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP2 pid=65037)[0;0m INFO 12-05 18:36:18 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 278.050157 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP1 pid=65035)[0;0m INFO 12-05 18:36:18 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 277.950803 seconds
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP6 pid=65045)[0;0m INFO 12-05 18:37:26 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_6_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP1 pid=65035)[0;0m INFO 12-05 18:37:26 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_1_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP0 pid=65033)[0;0m INFO 12-05 18:37:26 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP4 pid=65041)[0;0m INFO 12-05 18:37:26 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_4_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP5 pid=65043)[0;0m INFO 12-05 18:37:26 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_5_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP6 pid=65045)[0;0m INFO 12-05 18:37:26 [backends.py:550] Dynamo bytecode transform time: 65.53 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP7 pid=65047)[0;0m INFO 12-05 18:37:26 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_7_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP0 pid=65033)[0;0m INFO 12-05 18:37:26 [backends.py:550] Dynamo bytecode transform time: 65.52 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP2 pid=65037)[0;0m INFO 12-05 18:37:26 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_2_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP4 pid=65041)[0;0m INFO 12-05 18:37:26 [backends.py:550] Dynamo bytecode transform time: 65.50 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP7 pid=65047)[0;0m INFO 12-05 18:37:26 [backends.py:550] Dynamo bytecode transform time: 65.56 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP5 pid=65043)[0;0m INFO 12-05 18:37:26 [backends.py:550] Dynamo bytecode transform time: 65.45 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP3 pid=65038)[0;0m INFO 12-05 18:37:26 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_3_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP1 pid=65035)[0;0m INFO 12-05 18:37:26 [backends.py:550] Dynamo bytecode transform time: 65.48 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP2 pid=65037)[0;0m INFO 12-05 18:37:26 [backends.py:550] Dynamo bytecode transform time: 65.47 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP3 pid=65038)[0;0m INFO 12-05 18:37:26 [backends.py:550] Dynamo bytecode transform time: 65.51 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP4 pid=65041)[0;0m INFO 12-05 18:38:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 59.182 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP3 pid=65038)[0;0m INFO 12-05 18:38:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 60.457 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP5 pid=65043)[0;0m INFO 12-05 18:38:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 60.431 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP7 pid=65047)[0;0m INFO 12-05 18:38:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 60.466 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP6 pid=65045)[0;0m INFO 12-05 18:38:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 60.635 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP0 pid=65033)[0;0m INFO 12-05 18:38:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 60.562 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP2 pid=65037)[0;0m INFO 12-05 18:38:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 60.511 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP1 pid=65035)[0;0m INFO 12-05 18:38:31 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 61.182 s
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP5 pid=65043)[0;0m INFO 12-05 18:38:41 [monitor.py:34] torch.compile takes 65.45 s in total
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP3 pid=65038)[0;0m INFO 12-05 18:38:41 [monitor.py:34] torch.compile takes 65.51 s in total
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP4 pid=65041)[0;0m INFO 12-05 18:38:41 [monitor.py:34] torch.compile takes 65.50 s in total
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP0 pid=65033)[0;0m INFO 12-05 18:38:41 [monitor.py:34] torch.compile takes 65.52 s in total
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP7 pid=65047)[0;0m INFO 12-05 18:38:41 [monitor.py:34] torch.compile takes 65.56 s in total
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP6 pid=65045)[0;0m INFO 12-05 18:38:41 [monitor.py:34] torch.compile takes 65.53 s in total
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP2 pid=65037)[0;0m INFO 12-05 18:38:41 [monitor.py:34] torch.compile takes 65.47 s in total
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP1 pid=65035)[0;0m INFO 12-05 18:38:41 [monitor.py:34] torch.compile takes 65.48 s in total
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP3 pid=65038)[0;0m INFO 12-05 18:38:57 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP0 pid=65033)[0;0m INFO 12-05 18:38:57 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP6 pid=65045)[0;0m INFO 12-05 18:38:57 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP1 pid=65035)[0;0m INFO 12-05 18:38:57 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP5 pid=65043)[0;0m INFO 12-05 18:38:57 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP4 pid=65041)[0;0m INFO 12-05 18:38:57 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP2 pid=65037)[0;0m INFO 12-05 18:38:57 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=65027)[0;0m [1;36m(Worker_TP7 pid=65047)[0;0m INFO 12-05 18:38:57 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:864] GPU KV cache size: 1,643,200 tokens
[1;36m(EngineCore_DP0 pid=65027)[0;0m INFO 12-05 18:39:00 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.59x
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:09 [multiproc_executor.py:149] Worker proc VllmWorker-7 died unexpectedly, shutting down executor.
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 91, in __init__
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 215, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 74, in initialize_from_config
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 259, in collective_rpc
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]     result = get_response(w, dequeue_timeout,
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 239, in get_response
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]     status, result = w.worker_response_mq.dequeue(
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 507, in dequeue
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]     with self.acquire_read(timeout, cancel) as buf:
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]   File "/lcars/home/t/tongt1/.local/share/uv/python/cpython-3.10.18-linux-x86_64-gnu/lib/python3.10/contextlib.py", line 135, in __enter__
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]     return next(self.gen)
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 464, in acquire_read
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718]     raise RuntimeError("cancelled")
[1;36m(EngineCore_DP0 pid=65027)[0;0m ERROR 12-05 18:47:13 [core.py:718] RuntimeError: cancelled
INFO 12-05 18:47:37 [__init__.py:216] Automatically detected platform cuda.
INFO 12-05 18:47:42 [utils.py:328] non-default args: {'download_dir': '/nlpgpu/data/terry/ToolProj/src/models', 'dtype': 'bfloat16', 'seed': 3, 'max_model_len': 8192, 'tensor_parallel_size': 8, 'enable_prefix_caching': True, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-14B-Instruct'}
INFO 12-05 18:47:54 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM
INFO 12-05 18:47:54 [__init__.py:1815] Using max model len 8192
INFO 12-05 18:47:56 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:47:57 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:47:57 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir='/nlpgpu/data/terry/ToolProj/src/models', load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=3, served_model_name=Qwen/Qwen2.5-14B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:47:57 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:47:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_21222499'), local_subscribe_addr='ipc:///tmp/c333d16c-03bd-450d-bb3c-aaabe8832116', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_01c509e8'), local_subscribe_addr='ipc:///tmp/970a4dd2-7bd0-47de-bf8d-399be0df1faa', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a4d80514'), local_subscribe_addr='ipc:///tmp/b17a06c8-bc87-457b-ad57-353bdaeb793d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_19247e9b'), local_subscribe_addr='ipc:///tmp/70ac3761-3882-4484-bb33-59add4da2d5d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_23830341'), local_subscribe_addr='ipc:///tmp/12cdde2b-5007-4e7c-aa18-a74b29ba8ac3', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_97ad7d7c'), local_subscribe_addr='ipc:///tmp/a33f9312-bf6d-48ce-b6f0-8ae6adee4b13', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dacd1a0d'), local_subscribe_addr='ipc:///tmp/769ca215-146c-4ad1-8ac0-0afb64fa1297', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8dbb96b9'), local_subscribe_addr='ipc:///tmp/ce2a4bf8-0fb8-4283-9af1-ae07de9f59f9', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_03134d06'), local_subscribe_addr='ipc:///tmp/5e7de741-f520-4962-a061-a69b550388d7', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:16 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_60a7a729'), local_subscribe_addr='ipc:///tmp/13519120-4338-475b-a165-4d2071119704', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:18 [parallel_state.py:1165] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:18 [parallel_state.py:1165] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:18 [parallel_state.py:1165] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:18 [parallel_state.py:1165] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:18 [parallel_state.py:1165] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:18 [parallel_state.py:1165] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:18 [parallel_state.py:1165] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:48:18 [parallel_state.py:1165] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2350)[0;0m WARNING 12-05 18:48:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP3 pid=2361)[0;0m INFO 12-05 18:48:19 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP2 pid=2360)[0;0m INFO 12-05 18:48:19 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP6 pid=2368)[0;0m INFO 12-05 18:48:19 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP4 pid=2364)[0;0m INFO 12-05 18:48:19 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP5 pid=2366)[0;0m INFO 12-05 18:48:19 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP7 pid=2370)[0;0m INFO 12-05 18:48:19 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP1 pid=2358)[0;0m INFO 12-05 18:48:19 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP0 pid=2356)[0;0m INFO 12-05 18:48:19 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP2 pid=2360)[0;0m INFO 12-05 18:48:21 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP5 pid=2366)[0;0m INFO 12-05 18:48:21 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP7 pid=2370)[0;0m INFO 12-05 18:48:21 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP6 pid=2368)[0;0m INFO 12-05 18:48:21 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP0 pid=2356)[0;0m INFO 12-05 18:48:21 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP1 pid=2358)[0;0m INFO 12-05 18:48:21 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP3 pid=2361)[0;0m INFO 12-05 18:48:21 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP4 pid=2364)[0;0m INFO 12-05 18:48:21 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP5 pid=2366)[0;0m INFO 12-05 18:48:21 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP2 pid=2360)[0;0m INFO 12-05 18:48:21 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP0 pid=2356)[0;0m INFO 12-05 18:48:21 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP7 pid=2370)[0;0m INFO 12-05 18:48:21 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP6 pid=2368)[0;0m INFO 12-05 18:48:21 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP1 pid=2358)[0;0m INFO 12-05 18:48:21 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP3 pid=2361)[0;0m INFO 12-05 18:48:21 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP4 pid=2364)[0;0m INFO 12-05 18:48:21 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP7 pid=2370)[0;0m INFO 12-05 18:48:22 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP5 pid=2366)[0;0m INFO 12-05 18:48:22 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP6 pid=2368)[0;0m INFO 12-05 18:48:22 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP0 pid=2356)[0;0m INFO 12-05 18:48:22 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP4 pid=2364)[0;0m INFO 12-05 18:48:22 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP1 pid=2358)[0;0m INFO 12-05 18:48:22 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP3 pid=2361)[0;0m INFO 12-05 18:48:22 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP2 pid=2360)[0;0m INFO 12-05 18:48:22 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP3 pid=2361)[0;0m INFO 12-05 18:52:49 [default_loader.py:268] Loading weights took 264.95 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP5 pid=2366)[0;0m INFO 12-05 18:52:49 [default_loader.py:268] Loading weights took 265.93 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP0 pid=2356)[0;0m INFO 12-05 18:52:49 [default_loader.py:268] Loading weights took 265.64 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP2 pid=2360)[0;0m INFO 12-05 18:52:49 [default_loader.py:268] Loading weights took 265.13 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP6 pid=2368)[0;0m INFO 12-05 18:52:49 [default_loader.py:268] Loading weights took 265.35 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP7 pid=2370)[0;0m INFO 12-05 18:52:49 [default_loader.py:268] Loading weights took 265.41 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP1 pid=2358)[0;0m INFO 12-05 18:52:49 [default_loader.py:268] Loading weights took 265.58 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP4 pid=2364)[0;0m INFO 12-05 18:52:49 [default_loader.py:268] Loading weights took 265.41 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP2 pid=2360)[0;0m INFO 12-05 18:52:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 268.323110 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP1 pid=2358)[0;0m INFO 12-05 18:52:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 268.261528 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP3 pid=2361)[0;0m INFO 12-05 18:52:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 268.255102 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP5 pid=2366)[0;0m INFO 12-05 18:52:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 268.289901 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP7 pid=2370)[0;0m INFO 12-05 18:52:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 268.297899 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP6 pid=2368)[0;0m INFO 12-05 18:52:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 268.289077 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP4 pid=2364)[0;0m INFO 12-05 18:52:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 268.242894 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP0 pid=2356)[0;0m INFO 12-05 18:52:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 268.279867 seconds
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP4 pid=2364)[0;0m INFO 12-05 18:54:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_4_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP5 pid=2366)[0;0m INFO 12-05 18:54:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_5_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP1 pid=2358)[0;0m INFO 12-05 18:54:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_1_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP3 pid=2361)[0;0m INFO 12-05 18:54:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_3_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP2 pid=2360)[0;0m INFO 12-05 18:54:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_2_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP6 pid=2368)[0;0m INFO 12-05 18:54:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_6_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP7 pid=2370)[0;0m INFO 12-05 18:54:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_7_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP5 pid=2366)[0;0m INFO 12-05 18:54:00 [backends.py:550] Dynamo bytecode transform time: 64.69 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP2 pid=2360)[0;0m INFO 12-05 18:54:00 [backends.py:550] Dynamo bytecode transform time: 64.68 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP1 pid=2358)[0;0m INFO 12-05 18:54:00 [backends.py:550] Dynamo bytecode transform time: 64.72 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP4 pid=2364)[0;0m INFO 12-05 18:54:00 [backends.py:550] Dynamo bytecode transform time: 64.67 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP7 pid=2370)[0;0m INFO 12-05 18:54:00 [backends.py:550] Dynamo bytecode transform time: 64.55 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP6 pid=2368)[0;0m INFO 12-05 18:54:00 [backends.py:550] Dynamo bytecode transform time: 64.68 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP3 pid=2361)[0;0m INFO 12-05 18:54:00 [backends.py:550] Dynamo bytecode transform time: 64.74 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP0 pid=2356)[0;0m INFO 12-05 18:54:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP0 pid=2356)[0;0m INFO 12-05 18:54:00 [backends.py:550] Dynamo bytecode transform time: 64.96 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP2 pid=2360)[0;0m INFO 12-05 18:54:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 34.251 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP4 pid=2364)[0;0m INFO 12-05 18:54:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 34.269 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP0 pid=2356)[0;0m INFO 12-05 18:54:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 34.373 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP7 pid=2370)[0;0m INFO 12-05 18:54:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 35.099 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP6 pid=2368)[0;0m INFO 12-05 18:54:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 35.025 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP1 pid=2358)[0;0m INFO 12-05 18:54:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 35.156 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP3 pid=2361)[0;0m INFO 12-05 18:54:39 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 35.272 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP5 pid=2366)[0;0m INFO 12-05 18:54:40 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 35.525 s
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP1 pid=2358)[0;0m INFO 12-05 18:54:50 [monitor.py:34] torch.compile takes 64.72 s in total
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP2 pid=2360)[0;0m INFO 12-05 18:54:50 [monitor.py:34] torch.compile takes 64.68 s in total
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP0 pid=2356)[0;0m INFO 12-05 18:54:50 [monitor.py:34] torch.compile takes 64.96 s in total
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP6 pid=2368)[0;0m INFO 12-05 18:54:50 [monitor.py:34] torch.compile takes 64.68 s in total
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP7 pid=2370)[0;0m INFO 12-05 18:54:50 [monitor.py:34] torch.compile takes 64.55 s in total
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP3 pid=2361)[0;0m INFO 12-05 18:54:50 [monitor.py:34] torch.compile takes 64.74 s in total
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP4 pid=2364)[0;0m INFO 12-05 18:54:50 [monitor.py:34] torch.compile takes 64.67 s in total
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP5 pid=2366)[0;0m INFO 12-05 18:54:50 [monitor.py:34] torch.compile takes 64.69 s in total
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP6 pid=2368)[0;0m INFO 12-05 18:55:06 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP7 pid=2370)[0;0m INFO 12-05 18:55:06 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP5 pid=2366)[0;0m INFO 12-05 18:55:06 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP4 pid=2364)[0;0m INFO 12-05 18:55:06 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP2 pid=2360)[0;0m INFO 12-05 18:55:06 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP3 pid=2361)[0;0m INFO 12-05 18:55:06 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP0 pid=2356)[0;0m INFO 12-05 18:55:06 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=2350)[0;0m [1;36m(Worker_TP1 pid=2358)[0;0m INFO 12-05 18:55:06 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:864] GPU KV cache size: 1,643,200 tokens
[1;36m(EngineCore_DP0 pid=2350)[0;0m INFO 12-05 18:55:09 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.59x
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:08 [multiproc_executor.py:149] Worker proc VllmWorker-7 died unexpectedly, shutting down executor.
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 91, in __init__
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 215, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 74, in initialize_from_config
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 259, in collective_rpc
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]     result = get_response(w, dequeue_timeout,
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 239, in get_response
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]     status, result = w.worker_response_mq.dequeue(
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 507, in dequeue
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]     with self.acquire_read(timeout, cancel) as buf:
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]   File "/lcars/home/t/tongt1/.local/share/uv/python/cpython-3.10.18-linux-x86_64-gnu/lib/python3.10/contextlib.py", line 135, in __enter__
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]     return next(self.gen)
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 464, in acquire_read
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718]     raise RuntimeError("cancelled")
[1;36m(EngineCore_DP0 pid=2350)[0;0m ERROR 12-05 19:06:12 [core.py:718] RuntimeError: cancelled
INFO 12-05 19:06:36 [__init__.py:216] Automatically detected platform cuda.
INFO 12-05 19:06:41 [utils.py:328] non-default args: {'download_dir': '/nlpgpu/data/terry/ToolProj/src/models', 'dtype': 'bfloat16', 'seed': 4, 'max_model_len': 8192, 'tensor_parallel_size': 8, 'enable_prefix_caching': True, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-14B-Instruct'}
INFO 12-05 19:06:53 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM
INFO 12-05 19:06:53 [__init__.py:1815] Using max model len 8192
INFO 12-05 19:06:55 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:06:56 [core.py:654] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:06:56 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen2.5-14B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-14B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir='/nlpgpu/data/terry/ToolProj/src/models', load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=4, served_model_name=Qwen/Qwen2.5-14B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:06:56 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:06:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_7e52a778'), local_subscribe_addr='ipc:///tmp/18be377a-9d7f-4bc6-a05e-3763061133bd', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2b432cf1'), local_subscribe_addr='ipc:///tmp/182a4e10-6121-4d5a-84aa-9e4014a866de', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2553c9a0'), local_subscribe_addr='ipc:///tmp/f25ac355-ba4a-48bf-bb57-0af184a6547f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ade8cd37'), local_subscribe_addr='ipc:///tmp/03012a1e-57fa-4b79-af70-6ea1402fc14c', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_224a3c67'), local_subscribe_addr='ipc:///tmp/a03fc96d-25dc-408f-a7df-a1a4fe38f763', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b5bc696f'), local_subscribe_addr='ipc:///tmp/94529c38-0c92-4799-82e2-b1281ad7bd9f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2ee7069a'), local_subscribe_addr='ipc:///tmp/8173001b-6c09-44ef-8210-a52906efce38', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_360bd81f'), local_subscribe_addr='ipc:///tmp/7de59327-0202-4bae-a7eb-ad285567064b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_aad9c69b'), local_subscribe_addr='ipc:///tmp/9e09a130-6b79-438c-8758-193a84c4b6e4', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [__init__.py:1433] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:14 [pynccl.py:70] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:16 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:16 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:16 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:16 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:16 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:16 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:16 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:16 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_2c4c9fd3'), local_subscribe_addr='ipc:///tmp/08f295a8-6573-461f-849e-8c3d37787217', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:16 [parallel_state.py:1165] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:16 [parallel_state.py:1165] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:17 [parallel_state.py:1165] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:17 [parallel_state.py:1165] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:17 [parallel_state.py:1165] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:17 [parallel_state.py:1165] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:17 [parallel_state.py:1165] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:07:17 [parallel_state.py:1165] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
[1;36m(EngineCore_DP0 pid=3885)[0;0m WARNING 12-05 19:07:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP1 pid=3893)[0;0m INFO 12-05 19:07:17 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP3 pid=3896)[0;0m INFO 12-05 19:07:17 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP2 pid=3895)[0;0m INFO 12-05 19:07:17 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP5 pid=3901)[0;0m INFO 12-05 19:07:17 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP7 pid=3905)[0;0m INFO 12-05 19:07:17 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP4 pid=3899)[0;0m INFO 12-05 19:07:17 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP6 pid=3903)[0;0m INFO 12-05 19:07:17 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP0 pid=3891)[0;0m INFO 12-05 19:07:17 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-14B-Instruct...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP6 pid=3903)[0;0m INFO 12-05 19:07:19 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP7 pid=3905)[0;0m INFO 12-05 19:07:19 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP4 pid=3899)[0;0m INFO 12-05 19:07:19 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP3 pid=3896)[0;0m INFO 12-05 19:07:19 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP2 pid=3895)[0;0m INFO 12-05 19:07:19 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP5 pid=3901)[0;0m INFO 12-05 19:07:19 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP1 pid=3893)[0;0m INFO 12-05 19:07:19 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP0 pid=3891)[0;0m INFO 12-05 19:07:19 [gpu_model_runner.py:2370] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP7 pid=3905)[0;0m INFO 12-05 19:07:19 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP4 pid=3899)[0;0m INFO 12-05 19:07:19 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP3 pid=3896)[0;0m INFO 12-05 19:07:19 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP5 pid=3901)[0;0m INFO 12-05 19:07:19 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP2 pid=3895)[0;0m INFO 12-05 19:07:19 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP6 pid=3903)[0;0m INFO 12-05 19:07:19 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP1 pid=3893)[0;0m INFO 12-05 19:07:19 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP0 pid=3891)[0;0m INFO 12-05 19:07:19 [cuda.py:362] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP5 pid=3901)[0;0m INFO 12-05 19:07:21 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP1 pid=3893)[0;0m INFO 12-05 19:07:21 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP3 pid=3896)[0;0m INFO 12-05 19:07:21 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP2 pid=3895)[0;0m INFO 12-05 19:07:21 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP6 pid=3903)[0;0m INFO 12-05 19:07:21 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP4 pid=3899)[0;0m INFO 12-05 19:07:21 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP7 pid=3905)[0;0m INFO 12-05 19:07:21 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP0 pid=3891)[0;0m INFO 12-05 19:07:21 [weight_utils.py:348] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP0 pid=3891)[0;0m INFO 12-05 19:11:49 [default_loader.py:268] Loading weights took 266.68 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP3 pid=3896)[0;0m INFO 12-05 19:11:49 [default_loader.py:268] Loading weights took 266.96 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP5 pid=3901)[0;0m INFO 12-05 19:11:49 [default_loader.py:268] Loading weights took 266.70 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP2 pid=3895)[0;0m INFO 12-05 19:11:49 [default_loader.py:268] Loading weights took 267.05 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP1 pid=3893)[0;0m INFO 12-05 19:11:49 [default_loader.py:268] Loading weights took 267.33 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP7 pid=3905)[0;0m INFO 12-05 19:11:49 [default_loader.py:268] Loading weights took 266.82 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP6 pid=3903)[0;0m INFO 12-05 19:11:49 [default_loader.py:268] Loading weights took 267.21 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP4 pid=3899)[0;0m INFO 12-05 19:11:49 [default_loader.py:268] Loading weights took 267.66 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP5 pid=3901)[0;0m INFO 12-05 19:11:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 269.721821 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP2 pid=3895)[0;0m INFO 12-05 19:11:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 269.742509 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP1 pid=3893)[0;0m INFO 12-05 19:11:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 269.733913 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP0 pid=3891)[0;0m INFO 12-05 19:11:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 269.287345 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP7 pid=3905)[0;0m INFO 12-05 19:11:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 269.761201 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP3 pid=3896)[0;0m INFO 12-05 19:11:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 269.708444 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP4 pid=3899)[0;0m INFO 12-05 19:11:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 269.939160 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP6 pid=3903)[0;0m INFO 12-05 19:11:53 [gpu_model_runner.py:2392] Model loading took 3.4611 GiB and 269.776418 seconds
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP5 pid=3901)[0;0m INFO 12-05 19:13:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_5_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP6 pid=3903)[0;0m INFO 12-05 19:13:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_6_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP7 pid=3905)[0;0m INFO 12-05 19:13:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_7_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP1 pid=3893)[0;0m INFO 12-05 19:13:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_1_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP5 pid=3901)[0;0m INFO 12-05 19:13:00 [backends.py:550] Dynamo bytecode transform time: 64.89 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP6 pid=3903)[0;0m INFO 12-05 19:13:00 [backends.py:550] Dynamo bytecode transform time: 64.88 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP1 pid=3893)[0;0m INFO 12-05 19:13:00 [backends.py:550] Dynamo bytecode transform time: 64.89 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP4 pid=3899)[0;0m INFO 12-05 19:13:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_4_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP0 pid=3891)[0;0m INFO 12-05 19:13:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP2 pid=3895)[0;0m INFO 12-05 19:13:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_2_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP4 pid=3899)[0;0m INFO 12-05 19:13:00 [backends.py:550] Dynamo bytecode transform time: 64.92 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP0 pid=3891)[0;0m INFO 12-05 19:13:00 [backends.py:550] Dynamo bytecode transform time: 64.89 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP7 pid=3905)[0;0m INFO 12-05 19:13:00 [backends.py:550] Dynamo bytecode transform time: 64.89 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP2 pid=3895)[0;0m INFO 12-05 19:13:00 [backends.py:550] Dynamo bytecode transform time: 64.91 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP3 pid=3896)[0;0m INFO 12-05 19:13:00 [backends.py:539] Using cache directory: /lcars/home/t/tongt1/.cache/vllm/torch_compile_cache/a3fc4c338d/rank_3_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP3 pid=3896)[0;0m INFO 12-05 19:13:00 [backends.py:550] Dynamo bytecode transform time: 64.94 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP3 pid=3896)[0;0m INFO 12-05 19:13:36 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 32.275 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP1 pid=3893)[0;0m INFO 12-05 19:13:37 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 33.280 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP6 pid=3903)[0;0m INFO 12-05 19:13:37 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 33.151 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP7 pid=3905)[0;0m INFO 12-05 19:13:37 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 33.236 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP4 pid=3899)[0;0m INFO 12-05 19:13:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 33.729 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP2 pid=3895)[0;0m INFO 12-05 19:13:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 33.703 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP5 pid=3901)[0;0m INFO 12-05 19:13:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 34.273 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP0 pid=3891)[0;0m INFO 12-05 19:13:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 34.329 s
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP6 pid=3903)[0;0m INFO 12-05 19:13:48 [monitor.py:34] torch.compile takes 64.88 s in total
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP1 pid=3893)[0;0m INFO 12-05 19:13:48 [monitor.py:34] torch.compile takes 64.89 s in total
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP7 pid=3905)[0;0m INFO 12-05 19:13:48 [monitor.py:34] torch.compile takes 64.89 s in total
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP4 pid=3899)[0;0m INFO 12-05 19:13:48 [monitor.py:34] torch.compile takes 64.92 s in total
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP3 pid=3896)[0;0m INFO 12-05 19:13:48 [monitor.py:34] torch.compile takes 64.94 s in total
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP2 pid=3895)[0;0m INFO 12-05 19:13:48 [monitor.py:34] torch.compile takes 64.91 s in total
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP5 pid=3901)[0;0m INFO 12-05 19:13:48 [monitor.py:34] torch.compile takes 64.89 s in total
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP0 pid=3891)[0;0m INFO 12-05 19:13:49 [monitor.py:34] torch.compile takes 64.89 s in total
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP5 pid=3901)[0;0m INFO 12-05 19:14:04 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP6 pid=3903)[0;0m INFO 12-05 19:14:04 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP4 pid=3899)[0;0m INFO 12-05 19:14:04 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP2 pid=3895)[0;0m INFO 12-05 19:14:04 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP1 pid=3893)[0;0m INFO 12-05 19:14:04 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP0 pid=3891)[0;0m INFO 12-05 19:14:04 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP3 pid=3896)[0;0m INFO 12-05 19:14:04 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=3885)[0;0m [1;36m(Worker_TP7 pid=3905)[0;0m INFO 12-05 19:14:04 [gpu_worker.py:298] Available KV cache memory: 37.61 GiB
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:864] GPU KV cache size: 1,643,280 tokens
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.60x
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:864] GPU KV cache size: 1,643,200 tokens
[1;36m(EngineCore_DP0 pid=3885)[0;0m INFO 12-05 19:14:07 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 200.59x
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:07 [multiproc_executor.py:149] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 709, in run_engine_core
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 505, in __init__
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 91, in __init__
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 215, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]     self.model_executor.initialize_from_config(kv_cache_configs)
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 74, in initialize_from_config
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]     self.collective_rpc("compile_or_warm_up_model")
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 259, in collective_rpc
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]     result = get_response(w, dequeue_timeout,
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 239, in get_response
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]     status, result = w.worker_response_mq.dequeue(
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 507, in dequeue
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]     with self.acquire_read(timeout, cancel) as buf:
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]   File "/lcars/home/t/tongt1/.local/share/uv/python/cpython-3.10.18-linux-x86_64-gnu/lib/python3.10/contextlib.py", line 135, in __enter__
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]     return next(self.gen)
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]   File "/mnt/nlpgpu-io1/data/terry/ToolProj/.venv/lib/python3.10/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 464, in acquire_read
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718]     raise RuntimeError("cancelled")
[1;36m(EngineCore_DP0 pid=3885)[0;0m ERROR 12-05 19:16:11 [core.py:718] RuntimeError: cancelled
