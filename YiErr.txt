cpu-bind=MASK - nlpgpu07, task  0  0 [3302]: mask 0x100000001 set
`torch_dtype` is deprecated! Use `dtype` instead!
Traceback (most recent call last):
  File "/mnt/nlpgpu-io1/data/terry/ToolProj/src/exps_performance/main.py", line 153, in <module>
    run(args)
  File "/mnt/nlpgpu-io1/data/terry/ToolProj/src/exps_performance/main.py", line 39, in run
    client = llm(args)
  File "/mnt/nlpgpu-io1/data/terry/ToolProj/src/exps_performance/llm.py", line 268, in llm
    client = VLLMClient(
  File "/mnt/nlpgpu-io1/data/terry/ToolProj/src/exps_performance/llm.py", line 134, in __init__
    self.llm = VLLMEngine(
  File "/mnt/nlpgpu-io1/data/terry/ToolProj/.pixi/envs/default/lib/python3.10/site-packages/vllm/utils.py", line 1031, in inner
    return fn(*args, **kwargs)
  File "/mnt/nlpgpu-io1/data/terry/ToolProj/.pixi/envs/default/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 242, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/mnt/nlpgpu-io1/data/terry/ToolProj/.pixi/envs/default/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 513, in from_engine_args
    vllm_config = engine_args.create_engine_config(usage_context)
  File "/mnt/nlpgpu-io1/data/terry/ToolProj/.pixi/envs/default/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1206, in create_engine_config
    model_config = self.create_model_config()
  File "/mnt/nlpgpu-io1/data/terry/ToolProj/.pixi/envs/default/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1121, in create_model_config
    return ModelConfig(
  File "/mnt/nlpgpu-io1/data/terry/ToolProj/.pixi/envs/default/lib/python3.10/site-packages/vllm/config.py", line 388, in __init__
    self.max_model_len = _get_and_verify_max_len(
  File "/mnt/nlpgpu-io1/data/terry/ToolProj/.pixi/envs/default/lib/python3.10/site-packages/vllm/config.py", line 2729, in _get_and_verify_max_len
    raise ValueError(
ValueError: User-specified max_model_len (8192) is greater than the derived max_model_len (max_position_embeddings=4096 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. To allow overriding this maximum, set the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
